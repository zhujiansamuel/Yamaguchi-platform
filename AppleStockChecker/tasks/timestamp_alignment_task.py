from __future__ import annotations
import logging
from datetime import timedelta
from django.utils import timezone
from django.utils.dateparse import parse_datetime
from AppleStockChecker.utils.timestamp_alignment_task import (
    collect_items_for_psta,
    nearest_past_minute_iso,
    notify_progress_all,
    notify_batch_items_all,
    notify_batch_done_all,
    FeatureWriter,
    FeatureRecord,
)
from typing import Any, Dict, List, Optional
from collections import Counter, defaultdict
from celery import shared_task, chord, chain
from django.core.exceptions import ObjectDoesNotExist, ValidationError
from django.db import transaction, IntegrityError
from decimal import Decimal, ROUND_HALF_UP
import os
import logging
from typing import Any, Callable, Dict, Iterable, Tuple, Optional, Union
from typing import Optional, Dict, Any, List

logger = logging.getLogger(__name__)

# ç¯å¢ƒå¼€å…³ï¼šå‚æ•°ä¸¥æ ¼åº¦ & ç‰ˆæœ¬é˜ˆå€¼
PARAM_STRICT = os.getenv("PSTA_PARAM_STRICT", "warn").strip().lower()  # ignore|warn|error
MIN_ACCEPTED_TASK_VER = int(os.getenv("PSTA_MIN_ACCEPTED_VER", "0"))  # å°äºæ­¤ç‰ˆæœ¬ç›´æ¥æŠ¥é”™ï¼ˆå¯é€‰ï¼‰

# ---- å°å·¥å…·ï¼šç±»å‹è½¬æ¢ ----
_TRUE = {"1", "true", "t", "y", "yes", "on"}
_FALSE = {"0", "false", "f", "n", "no", "off"}


def to_bool(x: Any) -> bool:
    if isinstance(x, bool): return x
    if isinstance(x, (int, float)): return bool(int(x))
    if isinstance(x, str):
        s = x.strip().lower()
        if s in _TRUE: return True
        if s in _FALSE: return False
    raise ValueError(f"cannot coerce to bool: {x!r}")


def to_int(x: Any) -> int:
    if x is None: return None  # å…è®¸ä¸Šå±‚å†³å®šæ˜¯å¦å¿…å¡«
    return int(x)


def ensure_list(x: Any) -> list:
    if x is None: return []
    return list(x) if not isinstance(x, list) else x


def _isinstance_soft(val: Any, typ: Union[type, Tuple[type, ...]]) -> bool:
    # å…è®¸ä¼ å…¥ (int, str) è¿™æ ·çš„ tuple
    try:
        return isinstance(val, typ)
    except TypeError:
        # ä¸åšæ·±åº¦æ ¡éªŒ
        return True


# ---- å®ˆå«æ ¸å¿ƒ ----
def guard_params(
        task_name: str,
        incoming: Dict[str, Any],
        *,
        required: Dict[str, Union[type, Tuple[type, ...]]],
        optional: Dict[str, Union[type, Tuple[type, ...]]] = None,
        defaults: Dict[str, Any] = None,
        aliases: Dict[str, str] = None,  # å½¢å‚æ›´åï¼šold -> newï¼ˆä»…é¡¶å±‚ï¼‰
        coerce: Dict[str, Callable[[Any], Any]] = None,
        task_ver_field: str = "task_ver",
        expected_ver: Optional[int] = None,  # æ¨èï¼šä¸ producer åŒæ­¥å¡«å†™
        notify: Optional[Callable[[dict], Any]] = None,  # ä¾‹å¦‚ notify_progress_all
) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    """
    è¿”å› (normalized_kwargs, meta)
    - å¯¹æœªçŸ¥å‚æ•°æŒ‰ç­–ç•¥ ignore/warn/error å¤„ç†
    - è¿›è¡Œåˆ«åè¿ç§»ã€é»˜è®¤å€¼å¡«å……ã€ç±»å‹è½¬æ¢ä¸æ ¡éªŒ
    - æ£€æŸ¥ task_verï¼ˆè‹¥æä¾›ï¼‰
    """
    optional = optional or {}
    defaults = defaults or {}
    aliases = aliases or {}
    coerce = coerce or {}

    kw = dict(incoming)  # æµ…æ‹·è´

    # 1) é¡¶å±‚åˆ«åè¿ç§»ï¼ˆold->newï¼‰
    used_aliases = {}
    for old, new in aliases.items():
        if old in kw and new not in kw:
            kw[new] = kw.pop(old)
            used_aliases[old] = new

    # 2) æœªçŸ¥å‚æ•°å¤„ç†ç­–ç•¥
    declared = set(required.keys()) | set(optional.keys()) | {task_ver_field}
    unknown_keys = sorted(k for k in kw.keys() if k not in declared)
    if unknown_keys:
        msg = f"[{task_name}] unknown params: {unknown_keys} (strict={PARAM_STRICT})"
        if PARAM_STRICT == "error":
            raise TypeError(msg)
        elif PARAM_STRICT == "warn":
            logger.warning(msg)
            if notify:
                try:
                    notify({"type": "param_compat_warning", "task": task_name, "unknown": unknown_keys})
                except Exception:
                    pass
        # ignore: ä»€ä¹ˆä¹Ÿä¸åš

    # 3) é»˜è®¤å€¼
    for k, v in defaults.items():
        if kw.get(k) is None:
            kw[k] = v

    # 4) ç±»å‹è½¬æ¢ï¼ˆcoerceï¼‰
    for k, fn in coerce.items():
        if k in kw and kw[k] is not None:
            try:
                kw[k] = fn(kw[k])
            except Exception as e:
                raise ValueError(f"[{task_name}] bad param '{k}': {e}")

    # 5) å¿…å¡«ä¸å¯é€‰çš„ç±»å‹æ ¡éªŒ
    for k, typ in required.items():
        if kw.get(k) is None:
            raise ValueError(f"[{task_name}] missing required param: '{k}'")
        if not _isinstance_soft(kw[k], typ):
            raise TypeError(f"[{task_name}] param '{k}' type error: got {type(kw[k]).__name__}, expect {typ}")

    for k, typ in optional.items():
        if k in kw and kw[k] is not None and not _isinstance_soft(kw[k], typ):
            raise TypeError(f"[{task_name}] param '{k}' type error: got {type(kw[k]).__name__}, expect {typ}")

    # 6) ä»»åŠ¡ç‰ˆæœ¬æ¡æ‰‹ï¼ˆå¯é€‰ä½†æ¨èï¼‰
    tv = kw.get(task_ver_field)
    ver_meta = {"task_ver": tv, "expected_ver": expected_ver, "min_accepted": MIN_ACCEPTED_TASK_VER}
    try:
        if tv is not None:
            tv = int(tv)
            kw[task_ver_field] = tv
            if tv < MIN_ACCEPTED_TASK_VER:
                raise ValueError(f"[{task_name}] task_ver {tv} < min accepted {MIN_ACCEPTED_TASK_VER}")
            if expected_ver is not None and tv != expected_ver and PARAM_STRICT != "ignore":
                msg = f"[{task_name}] task_ver mismatch: got {tv}, expect {expected_ver}"
                logger.warning(msg)
                if notify:
                    try:
                        notify({"type": "task_version_mismatch", "task": task_name, "got": tv, "expect": expected_ver})
                    except Exception:
                        pass
        else:
            if PARAM_STRICT == "error":
                raise ValueError(f"[{task_name}] missing '{task_ver_field}'")
            elif PARAM_STRICT == "warn":
                logger.warning(f"[{task_name}] missing '{task_ver_field}'")
    except Exception as e:
        # ç‰ˆæœ¬é”™è¯¯ç›´æ¥æŠ›å‡º
        raise

    # åªå›ä¼ å£°æ˜å­—æ®µï¼ˆé¿å…æŠŠæœªçŸ¥å­—æ®µç»§ç»­å¾€ä¸‹ä¼ ï¼‰
    filtered = {k: kw[k] for k in declared if k in kw}
    meta = {"unknown": unknown_keys, "aliases_used": used_aliases, "version": ver_meta}
    return filtered, meta


#
from decimal import Decimal, ROUND_HALF_UP
from collections import Counter
from django.db import transaction, IntegrityError


# === æ—¶é—´ / æ•°å€¼å·¥å…· ===

def _to_aware(s: str):
    """ISO å­—ç¬¦ä¸² -> timezone aware datetime"""
    from django.utils.dateparse import parse_datetime
    from django.utils.timezone import make_aware, is_naive

    dt = parse_datetime(s)
    if dt is None:
        raise ValueError(f"bad datetime iso: {s}")
    return make_aware(dt) if is_naive(dt) else dt


def _tz_offset_str(dt):
    """æ ¼å¼æˆ +09:00 è¿™ç§å­—ç¬¦ä¸²"""
    z = dt.strftime("%z")  # e.g. +0900
    return z[:-2] + ":" + z[-2:]


def _d4(x):
    """ä¿ç•™ 2 ä½å°æ•°å¹¶å››èˆäº”å…¥ï¼ˆä½ åŸæ¥çš„å®ç°ï¼‰"""
    if x is None:
        return None
    return Decimal(str(x)).quantize(Decimal("0.01"), rounding=ROUND_HALF_UP)


def _quantile(sorted_vals, p: float):
    """æœ€è¿‘é‚»åˆ†ä½æ•°ï¼ˆsorted_vals å¿…é¡»å‡åºï¼‰ã€‚"""
    if not sorted_vals:
        return None
    n = len(sorted_vals)
    if n == 1:
        return float(sorted_vals[0])
    k = int(round((n - 1) * p))
    k = 0 if k < 0 else (n - 1 if k > n - 1 else k)
    return float(sorted_vals[k])


def _pop_std(vals):
    """æ€»ä½“æ ‡å‡†å·®ï¼›N<=1 è¿”å› 0."""
    n = len(vals)
    if n <= 1:
        return 0.0
    mu = sum(vals) / n
    s2 = sum((v - mu) ** 2 for v in vals) / n
    return (s2 ** 0.5)


def _filter_outliers_by_mean_band(vals, lower_factor=0.5, upper_factor=1.5):
    """
    æŒ‰â€œç›¸å¯¹å¹³å‡å€¼â€è¿‡æ»¤å¼‚å¸¸å€¼ï¼š
    - å…ˆç®—åŸå§‹å‡å€¼ mï¼›
    - ä¿ç•™ [m*lower_factor, m*upper_factor] åŒºé—´å†…çš„å€¼ï¼›
    - å¦‚æœå…¨éƒ¨è¢«è¿‡æ»¤æ‰ï¼Œåˆ™å›é€€åˆ°åŸå§‹åˆ—è¡¨ã€‚
    è¿”å› (filtered_vals, m, low, high)ã€‚
    """
    if not vals:
        return [], None, None, None
    m = sum(vals) / len(vals)
    if m <= 0:
        # æç«¯æƒ…å†µï¼ˆä¸å¤ªä¼šå‘ç”Ÿï¼‰ï¼Œç›´æ¥ä¸æ»¤
        return list(vals), m, None, None
    low = m * lower_factor
    high = m * upper_factor
    filtered = [v for v in vals if low <= v <= high]
    if not filtered:
        # å…¨è¢«åˆ¤æˆå¼‚å¸¸ï¼Œå°±ç”¨åŸå§‹å€¼ï¼Œé¿å…æ•´ç»„ä¸¢å¤±
        return list(vals), m, low, high
    return filtered, m, low, high


# ====== ç»Ÿä¸€çš„ FeatureSnapshot å®‰å…¨ upsertï¼ˆå…¨å±€å·¥å…·å‡½æ•°ï¼‰ ======
def safe_upsert_feature_snapshot(
    *,
    bucket,
    scope,
    name,
    version,
    value,
    is_final,
    max_retries: int = 2,
):
    """
    FeatureSnapshot çš„ LWWï¼ˆlast-write-winsï¼‰å®‰å…¨ upsertã€‚

    - value ä¼šå…ˆèµ° _d4 å†è½¬ float
    - å¦‚æœå¹¶å‘æ’å…¥æ’å”¯ä¸€é”®ï¼Œä¼šè‡ªåŠ¨é‡è¯•å¹¶è¦†ç›–
    """
    from AppleStockChecker.models import FeatureSnapshot  # å±€éƒ¨ import é¿å…å¾ªç¯ä¾èµ–

    value = float(_d4(value))
    for attempt in range(max_retries + 1):
        try:
            with transaction.atomic():
                # å…ˆé”å·²æœ‰è¡Œï¼Œå­˜åœ¨åˆ™ç›´æ¥è¦†ç›–ï¼ˆLWWï¼‰
                qs = (
                    FeatureSnapshot.objects
                    .select_for_update()
                    .filter(bucket=bucket, scope=scope, name=name, version=version)
                )
                obj = qs.first()
                if obj:
                    obj.value = value
                    obj.is_final = bool(is_final)  # â† è¦†ç›–ï¼Œè€Œé OR
                    obj.save(update_fields=["value", "is_final"])
                    return obj
                # ä¸å­˜åœ¨åˆ™åˆ›å»º
                return FeatureSnapshot.objects.create(
                    bucket=bucket,
                    scope=scope,
                    name=name,
                    version=version,
                    value=value,
                    is_final=bool(is_final),
                )
        except IntegrityError:
            if attempt >= max_retries:
                raise
            # å¹¶å‘æ’å…¥æ’å”¯ä¸€é”®ï¼Œé‡è¯•æ—¶ä¼šè¯»åˆ°é‚£è¡Œå†è¦†ç›–
            continue





# å·²æœ‰çš„:
# _to_aware, _tz_offset_str, _d4, _quantile, _pop_std, _filter_outliers_by_mean_band, safe_upsert_feature_snapshot
# è¿™é‡Œæ–°å¢å‡ ç±»ï¼šSMA / EMA / WMA / fetch_prev_base


def _ema_from_series(series_old_to_new: List[float], alpha: float) -> float:
    """æ—§->æ–°åºåˆ— + alphaï¼Œè¿”å› EMA å€¼ã€‚"""
    if not series_old_to_new:
        return 0.0
    ema = float(series_old_to_new[0])
    for v in series_old_to_new[1:]:
        ema = alpha * float(v) + (1.0 - alpha) * ema
    return ema


def _sma(series_old_to_new: List[float], window: int) -> Optional[float]:
    """ç®€å•ç§»åŠ¨å¹³å‡ï¼ˆæ—§->æ–°åºåˆ—ï¼‰ï¼Œè¿”å› None è¡¨ç¤ºæ— æ•°æ®ã€‚"""
    if not series_old_to_new:
        return None
    w = max(1, int(window))
    s = series_old_to_new[-w:] if w < len(series_old_to_new) else series_old_to_new
    return sum(s) / float(len(s))


def _wma_linear(series_old_to_new: List[float], window: int) -> Optional[float]:
    """çº¿æ€§æƒé‡ç§»åŠ¨å¹³å‡ï¼Œè¶Šæ–°çš„æƒé‡è¶Šå¤§ã€‚"""
    if not series_old_to_new:
        return None
    w = max(1, int(window))
    s = series_old_to_new[-w:] if w < len(series_old_to_new) else series_old_to_new
    n = len(s)
    weights = list(range(1, n + 1))
    denom = float(sum(weights))
    return sum(v * w for v, w in zip(s, weights)) / denom if denom > 0 else None


def _fetch_prev_base(scope: str, base_name: str, base_version: str, limit: int, anchor_dt):
    """
    è¯»å–å†å²â€œåŸºå€¼â€åºåˆ—ï¼ˆä¸åŒ…å«å½“å‰ x_tï¼‰ï¼ŒæŒ‰æ—¶é—´ä»æ–°åˆ°æ—§å– limit æ¡ï¼š
    - overall:iphone:<id>  -> OverallBar.mean
    - cohort:<slug>        -> CohortBar.mean
    - å…¶ä»– scope           -> FeatureSnapshot(base_name)
    """
    from AppleStockChecker.models import OverallBar, CohortBar, FeatureSnapshot

    if scope.startswith("overall:iphone:"):
        ipid = int(scope.rsplit(":", 1)[-1])
        rows = (
            OverallBar.objects
            .filter(iphone_id=ipid, bucket__lt=anchor_dt)
            .order_by("-bucket")
            .values_list("mean", flat=True)[:limit]
        )
        return [float(v) for v in rows if v is not None]

    if scope.startswith("cohort:"):
        slug = scope.split(":", 1)[1]
        rows = (
            CohortBar.objects
            .filter(cohort__slug=slug, bucket__lt=anchor_dt)
            .order_by("-bucket")
            .values_list("mean", flat=True)[:limit]
        )
        return [float(v) for v in rows if v is not None]

    rows = (
        FeatureSnapshot.objects
        .filter(scope=scope, name=base_name, version=base_version, bucket__lt=anchor_dt)
        .order_by("-bucket")
        .values_list("value", flat=True)[:limit]
    )
    return [float(v) for v in rows if v is not None]

def _agg_overallbar(
    *,
    ts_iso: str,
    ts_dt,
    rows: List[Dict[str, Any]],
    use_window: bool,
    bucket_start,
    bucket_end,
    is_final_bar: bool,
    agg_ctx: Dict[str, Any],
    ob_has_iphone: bool,
):
    """
    1) OverallBarï¼ˆå…¨éƒ¨åº— Ã— å„ iPhoneï¼‰
    """
    from statistics import median
    from AppleStockChecker.models import PurchasingShopTimeAnalysis, OverallBar

    overallbar_debug = {"agg": agg_ctx, "iphones": [], "skipped": False}

    try:
        if not ob_has_iphone:
            overallbar_debug["skipped"] = True
        else:
            bucket_iphone_ids = sorted({int(r.get("iphone_id")) for r in rows if r.get("iphone_id")})
            if not bucket_iphone_ids:
                overallbar_debug["skipped"] = True

            for ipid in bucket_iphone_ids:
                if use_window:
                    # çª—å£å†…ï¼šæ¯åº—æœ€åä¸€æ¡
                    qs_latest = (
                        PurchasingShopTimeAnalysis.objects
                        .filter(
                            iphone_id=ipid,
                            Timestamp_Time__gte=bucket_start,
                            Timestamp_Time__lt=bucket_end,
                            New_Product_Price__isnull=False,  # åªè¿‡æ»¤ç©ºå€¼ï¼Œä¸ç”¨å›ºå®šé˜ˆå€¼
                        )
                        .order_by("shop_id", "-Timestamp_Time")
                        .distinct("shop_id")
                    )
                    prices_raw = [
                        float(p)
                        for p in qs_latest.values_list("New_Product_Price", flat=True)
                        if p is not None
                    ]
                    shop_cnt = qs_latest.values("shop_id").count()
                    ob_bucket = bucket_start
                    reference_time = bucket_end  # ä½¿ç”¨æ¡¶ç»“æŸæ—¶é—´ä½œä¸ºå‚è€ƒæ—¶é—´
                else:
                    qs_latest = (
                        PurchasingShopTimeAnalysis.objects
                        .filter(
                            iphone_id=ipid,
                            Timestamp_Time=ts_dt,
                            New_Product_Price__isnull=False,  # åªè¿‡æ»¤ç©ºå€¼ï¼Œä¸ç”¨å›ºå®šé˜ˆå€¼
                        )
                        .values("shop_id", "New_Product_Price")
                    )
                    prices_raw = [
                        float(r["New_Product_Price"])
                        for r in qs_latest
                        if r["New_Product_Price"] is not None
                    ]
                    shop_cnt = qs_latest.values("shop_id").distinct().count()
                    ob_bucket = ts_dt
                    reference_time = ts_dt

                if not prices_raw:
                    continue

                # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨åŠ¨æ€ä»·æ ¼åŒºé—´è¿‡æ»¤æ˜æ˜¾é”™è¯¯çš„æ•°æ®
                price_min, price_max = get_dynamic_price_range(ipid, reference_time)
                prices = [p for p in prices_raw if price_min <= p <= price_max]

                if not prices:
                    # å¦‚æœåŠ¨æ€åŒºé—´è¿‡æ»¤åæ²¡æœ‰æ•°æ®ï¼Œè®°å½•è­¦å‘Šå¹¶ä½¿ç”¨åŸå§‹æ•°æ®
                    logger.warning(
                        f"åŠ¨æ€ä»·æ ¼è¿‡æ»¤åæ— æ•°æ®: iphone_id={ipid}, "
                        f"åŸå§‹æ ·æœ¬æ•°={len(prices_raw)}, "
                        f"åŒºé—´=[{price_min:.0f}, {price_max:.0f}]"
                    )
                    prices = prices_raw

                # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ç»Ÿè®¡æ–¹æ³•è¿‡æ»¤å¼‚å¸¸å€¼ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
                vals_raw = [float(p) for p in prices]
                vals_filtered, _, _, _ = _filter_outliers_by_mean_band(vals_raw)
                if not vals_filtered:
                    continue

                vals = sorted(vals_filtered)
                m_mean = sum(vals) / len(vals)
                m_median = float(median(vals))
                m_std = _pop_std(vals)
                p10 = _quantile(vals, 0.10)
                p90 = _quantile(vals, 0.90)
                dispersion = (p90 - p10) if (p10 is not None and p90 is not None) else 0.0

                OverallBar.objects.update_or_create(
                    bucket=ob_bucket,
                    iphone_id=ipid,
                    defaults=dict(
                        mean=_d4(m_mean),
                        median=_d4(m_median),
                        std=_d4(m_std) if m_std is not None else None,
                        shop_count=shop_cnt,
                        dispersion=_d4(dispersion),
                        is_final=is_final_bar,
                    ),
                )

                if len(overallbar_debug["iphones"]) < 5:
                    overallbar_debug["iphones"].append({
                        "iphone_id": ipid,
                        "shop_count": shop_cnt,
                        "mean": round(m_mean, 4),
                        "median": round(m_median, 4),
                        "std": (round(m_std, 4) if m_std is not None else None),
                        "dispersion": round(dispersion, 4),
                        "bucket": ob_bucket.isoformat(),
                        "is_final": is_final_bar,
                    })

            try:
                notify_progress_all(data={
                    "type": "overallbar_update",
                    "bucket": ts_iso,
                    "detail": overallbar_debug,
                })
            except Exception:
                pass

    except Exception as e:
        try:
            notify_progress_all(data={
                "type": "overallbar_error",
                "bucket": ts_iso,
                "error": repr(e),
                "agg": agg_ctx,
            })
        except Exception:
            pass


def _agg_cohortbar(
    *,
    ts_iso: str,
    ob_bucket,
    is_final_bar: bool,
    agg_ctx: Dict[str, Any],
    ob_has_iphone: bool,
):
    """
    2) CohortBarï¼ˆå…¨éƒ¨åº— Ã— ç»„åˆ iPhoneï¼‰
    """
    from statistics import median
    from AppleStockChecker.models import Cohort, CohortMember, OverallBar, CohortBar

    cohort_debug = {"agg": agg_ctx, "bucket": ts_iso, "cohorts": []}

    try:
        if ob_has_iphone:
            cohorts = list(Cohort.objects.all())
            for coh in cohorts:
                members = list(
                    CohortMember.objects
                    .filter(cohort=coh)
                    .values("iphone_id", "weight")
                )
                if not members:
                    continue

                member_ids = [m["iphone_id"] for m in members]
                weight_map = {
                    m["iphone_id"]: float(m.get("weight") or 1.0)
                    for m in members
                }

                ob_rows = list(
                    OverallBar.objects
                    .filter(bucket=ob_bucket, iphone_id__in=member_ids)
                    .values("iphone_id", "mean", "shop_count")
                )
                vals = [float(r["mean"]) for r in ob_rows if r.get("mean") is not None]
                if not vals:
                    continue

                denom = 0.0
                num = 0.0
                for r in ob_rows:
                    v = r.get("mean")
                    if v is None:
                        continue
                    w = weight_map.get(r["iphone_id"], 1.0) * float(r.get("shop_count") or 0.0)
                    denom += w
                    num += w * float(v)

                c_mean = (num / denom) if denom > 0 else (sum(vals) / len(vals))

                vals_sorted = sorted(vals)
                c_median = float(median(vals_sorted))
                c_std = _pop_std(vals_sorted)
                p10 = _quantile(vals_sorted, 0.10)
                p90 = _quantile(vals_sorted, 0.90)
                c_disp = (p90 - p10) if (p10 is not None and p90 is not None) else 0.0
                n_models = len(vals_sorted)
                shop_count_agg = sum(int(r.get("shop_count") or 0) for r in ob_rows)

                CohortBar.objects.update_or_create(
                    bucket=ob_bucket,
                    cohort=coh,
                    defaults=dict(
                        mean=_d4(c_mean),
                        median=_d4(c_median),
                        std=_d4(c_std) if c_std is not None else None,
                        n_models=n_models,
                        shop_count_agg=shop_count_agg,
                        dispersion=_d4(c_disp),
                        is_final=is_final_bar,
                    ),
                )

                if len(cohort_debug["cohorts"]) < 5:
                    cohort_debug["cohorts"].append({
                        "cohort": {"id": coh.id, "slug": getattr(coh, "slug", str(coh))},
                        "n_models": n_models,
                        "shop_count_agg": shop_count_agg,
                        "mean": round(c_mean, 4),
                        "median": round(c_median, 4),
                        "std": (round(c_std, 4) if c_std is not None else None),
                        "dispersion": round(c_disp, 4),
                        "bucket": ob_bucket.isoformat(),
                        "is_final": is_final_bar,
                    })

            try:
                notify_progress_all(data={
                    "type": "cohortbar_update",
                    "bucket": ts_iso,
                    "detail": cohort_debug,
                })
            except Exception:
                pass
        else:
            try:
                notify_progress_all(data={
                    "type": "cohortbar_skipped",
                    "bucket": ts_iso,
                    "reason": "OverallBar lacks iphone dimension; skip CohortBar to avoid collisions.",
                    "agg": agg_ctx,
                })
            except Exception:
                pass

    except Exception as e:
        try:
            notify_progress_all(data={
                "type": "cohortbar_error",
                "bucket": ts_iso,
                "error": repr(e),
                "agg": agg_ctx,
            })
        except Exception:
            pass


def _agg_feature_combos(
    *,
    ts_iso: str,
    ts_dt,
    rows: List[Dict[str, Any]],
    bucket_start,
    bucket_end,
    use_window: bool,
    anchor_bucket,
    agg_ctx: Dict[str, Any],
    is_final_bar: bool,
    writer,
):
    """
    3) å››ç±»ç»„åˆç»Ÿè®¡ï¼šå†™å…¥ FeatureSnapshotï¼ˆçª—å£å»é‡ + æ—¶æ•ˆæƒï¼‰
    - Case1: shop Ã— iphone
    - Case2: shopcohort Ã— iphone
    - Case3: shop Ã— cohort
    - Case4: shopcohort Ã— cohort
    """
    from django.conf import settings
    from AppleStockChecker.models import (
        PurchasingShopTimeAnalysis,
        Cohort, CohortMember,
        ShopWeightProfile, ShopWeightItem,
    )

    try:
        # â€”â€” é¢„å–æœ¬æ¡¶å‡ºç°è¿‡çš„ shop/iphone â€”â€” #
        # Bugä¿®å¤: çª—å£æ¨¡å¼æ—¶åº”è¯¥ä»çª—å£å†…çš„ PSTA æ•°æ®æå– shop/iphoneï¼Œè€Œä¸æ˜¯ä» rowsï¼ˆå•åˆ†é’Ÿæ•°æ®ï¼‰
        # åŸå› : è¾¹ç•Œåˆ†é’Ÿçš„ rows å¯èƒ½ä¸ºç©ºï¼Œä½†çª—å£å†…ä»æœ‰å†å²æ•°æ®éœ€è¦èšåˆ
        if use_window:
            # çª—å£æ¨¡å¼: ä»æ•°æ®åº“æŸ¥è¯¢çª—å£å†…æ‰€æœ‰çš„ shop_id å’Œ iphone_id
            shops_seen = sorted(set(
                PurchasingShopTimeAnalysis.objects
                .filter(
                    Timestamp_Time__gte=bucket_start,
                    Timestamp_Time__lt=bucket_end,
                    New_Product_Price__isnull=False,
                )
                .values_list('shop_id', flat=True)
                .distinct()
            ))
            iphones_seen = sorted(set(
                PurchasingShopTimeAnalysis.objects
                .filter(
                    Timestamp_Time__gte=bucket_start,
                    Timestamp_Time__lt=bucket_end,
                    New_Product_Price__isnull=False,
                )
                .values_list('iphone_id', flat=True)
                .distinct()
            ))
        else:
            # å•åˆ†é’Ÿæ¨¡å¼: ä» rows æå–ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            shops_seen = sorted({int(r.get("shop_id")) for r in rows if r.get("shop_id")})
            iphones_seen = sorted({int(r.get("iphone_id")) for r in rows if r.get("iphone_id")})

        # æ—¥å¿—è¾“å‡ºï¼šæ•°æ®æå–æƒ…å†µ
        logger.info(
            f"  ğŸ“Š [æ•°æ®æº] shops: {len(shops_seen)}ä¸ª, iphones: {len(iphones_seen)}ä¸ª | "
            f"æ¥æº: {'çª—å£PSTAæ•°æ®' if use_window else 'rowså‚æ•°'}"
        )

        if use_window:
            base_qs = (
                PurchasingShopTimeAnalysis.objects
                .filter(
                    Timestamp_Time__gte=bucket_start,
                    Timestamp_Time__lt=bucket_end,
                    shop_id__in=shops_seen,
                    iphone_id__in=iphones_seen,
                    New_Product_Price__isnull=False,  # åªè¿‡æ»¤ç©ºå€¼ï¼Œä¸ç”¨å›ºå®šé˜ˆå€¼
                )
                .order_by("shop_id", "iphone_id", "-Timestamp_Time")
                .distinct("shop_id", "iphone_id")
                .values("shop_id", "iphone_id", "New_Product_Price", "Timestamp_Time")
            )
            reference_time = bucket_end
        else:
            base_qs = (
                PurchasingShopTimeAnalysis.objects
                .filter(
                    Timestamp_Time=ts_dt,
                    shop_id__in=shops_seen,
                    iphone_id__in=iphones_seen,
                    New_Product_Price__isnull=False,  # åªè¿‡æ»¤ç©ºå€¼ï¼Œä¸ç”¨å›ºå®šé˜ˆå€¼
                )
                .values("shop_id", "iphone_id", "New_Product_Price", "Timestamp_Time")
            )
            reference_time = ts_dt

        # ä¸ºæ¯ä¸ª iphone_id è®¡ç®—åŠ¨æ€ä»·æ ¼åŒºé—´
        price_ranges = {}
        for iphone_id in iphones_seen:
            price_ranges[iphone_id] = get_dynamic_price_range(iphone_id, reference_time)

        # (shop, iphone) -> (last_price, last_ts)
        data_by_si: Dict[tuple, tuple] = {}
        filtered_count = 0
        total_count = 0
        for rec in base_qs:
            total_count += 1
            p = rec.get("New_Product_Price")
            t = rec.get("Timestamp_Time")
            if p is None:
                continue
            s = int(rec["shop_id"])
            i = int(rec["iphone_id"])

            # ä½¿ç”¨åŠ¨æ€ä»·æ ¼åŒºé—´è¿‡æ»¤
            price_min, price_max = price_ranges.get(i, (PRICE_FALLBACK_MIN, PRICE_FALLBACK_MAX))
            if not (price_min <= float(p) <= price_max):
                filtered_count += 1
                continue

            data_by_si[(s, i)] = (float(p), t)

        if filtered_count > 0:
            logger.info(
                f"ç‰¹å¾è®¡ç®—: åŠ¨æ€ä»·æ ¼è¿‡æ»¤ï¼Œæ€»è®°å½•={total_count}, è¿‡æ»¤={filtered_count}, "
                f"ä¿ç•™={total_count - filtered_count}"
            )

        # â€”â€” ç»Ÿè®¡å·¥å…· â€”â€” #
        def _stats(values):
            """è¿”å› (mean, median, std, dispersion, count)ï¼Œè‡ªåŠ¨æŒ‰å¹³å‡å€¼è¿‡æ»¤å¼‚å¸¸å€¼ã€‚"""
            if not values:
                return None
            vals_raw = [float(v) for v in values]
            vals_filtered, _, _, _ = _filter_outliers_by_mean_band(vals_raw)
            if not vals_filtered:
                return None
            vals = sorted(vals_filtered)
            n = len(vals)
            mean_v = sum(vals) / n
            med_v = vals[n // 2] if n % 2 else 0.5 * (vals[n // 2 - 1] + vals[n // 2])
            std_v = _pop_std(vals)
            p10 = _quantile(vals, 0.10)
            p90 = _quantile(vals, 0.90)
            disp_v = (p90 - p10) if (p10 is not None and p90 is not None) else 0.0
            return mean_v, med_v, std_v, disp_v, n

        # â€”â€” æ—¶æ•ˆæƒé‡ï¼ˆAGE_CAP + åŠè¡°æœŸ/çº¿æ€§ï¼‰ â€”â€” #
        AGE_CAP_MIN = float(getattr(settings, "PSTA_AGE_CAP_MIN", 12.0))  # è¶…è¿‡åˆ™ä¸è®¡
        RECENCY_HALF_LIFE_MIN = float(getattr(settings, "PSTA_RECENCY_HALF_LIFE_MIN", 6.0))  # æŒ‡æ•°åŠè¡°æœŸ
        RECENCY_DECAY = str(getattr(settings, "PSTA_RECENCY_DECAY", "exp")).lower()  # 'exp'|'linear'

        import math

        def recency_weight(last_ts, ref_end):
            if last_ts is None:
                return 0.0, None
            age_min = (ref_end - last_ts).total_seconds() / 60.0
            if age_min < 0:
                age_min = 0.0
            if age_min > AGE_CAP_MIN:
                return 0.0, age_min
            if RECENCY_DECAY == "linear":
                w = max(0.0, 1.0 - (age_min / max(AGE_CAP_MIN, 1e-6)))
            else:
                lam = math.log(2.0) / max(RECENCY_HALF_LIFE_MIN, 1e-6)
                w = math.exp(-lam * age_min)
            return float(w), age_min

        combo_debug = {
            "agg": agg_ctx,
            "bucket": ts_iso,
            "case1_shop_iphone": 0,
            "case2_shopcohort_iphone": 0,
            "case3_shop_cohortiphone": 0,
            "case4_shopcohort_cohortiphone": 0,
            "skipped": [],
            "samples": [],
        }

        # === CASE 1: å„åº— Ã— å„ iPhoneï¼ˆå•å€¼ï¼›ç”¨äºåŸå§‹æ›²çº¿ï¼‰ ===
        for (sid, iid), (v, t) in data_by_si.items():
            s = _stats([v])
            if not s:
                continue
            m, med, st, disp, n = s
            scope = f"shop:{sid}|iphone:{iid}"

            writer.write(scope, "mean", m)
            writer.write(scope, "median", med)
            writer.write(scope, "std", st)
            writer.write(scope, "dispersion", disp)
            writer.write(scope, "count", float(n))

            combo_debug["case1_shop_iphone"] += 1
            if len(combo_debug["samples"]) < 5:
                combo_debug["samples"].append({"case": 1, "scope": scope, "mean": round(m, 4)})

        # é¢„å–åº—é“ºç»„åˆï¼ˆShopWeightProfileï¼‰
        profiles = list(ShopWeightProfile.objects.all())
        prof_items = {
            prof.id: {
                it["shop_id"]: float(it.get("weight") or 1.0)
                for it in ShopWeightItem.objects.filter(profile=prof).values("shop_id", "weight")
            }
            for prof in profiles
        }
        has_shop_profile = any(bool(prof_items.get(p.id)) for p in profiles)

        # é¢„å–æœºå‹ç»„åˆï¼ˆCohortï¼‰
        cohorts = list(Cohort.objects.all())
        cmembers = {
            coh.id: {
                m["iphone_id"]: float(m.get("weight") or 1.0)
                for m in CohortMember.objects.filter(cohort=coh).values("iphone_id", "weight")
            }
            for coh in cohorts
        }

        # === CASE 2: ç»„åˆåº— Ã— å„ iPhoneï¼ˆåº—æƒ Ã— æ—¶æ•ˆæƒï¼‰ ===
        if has_shop_profile:
            for prof in profiles:
                sw = prof_items.get(prof.id, {})
                if not sw:
                    continue
                shops_in = set(sw.keys()) & set(shops_seen)
                if not shops_in:
                    continue

                for iid in iphones_seen:
                    vals, ages = [], []
                    wnum = wden = 0.0
                    for sid in shops_in:
                        pair = data_by_si.get((int(sid), int(iid)))
                        if not pair:
                            continue
                        v, t = pair
                        w_rec, age = recency_weight(t, bucket_end)
                        if w_rec <= 0.0:
                            continue
                        w_shop = float(sw.get(sid, 1.0))
                        w = w_shop * w_rec
                        vals.append(v)
                        if age is not None:
                            ages.append(age)
                        wnum += w * v
                        wden += w

                    if not vals:
                        continue

                    m_unw, med, st, disp, n = _stats(vals)
                    mean_w = (wnum / wden) if wden > 0 else m_unw
                    scope = f"shopcohort:{prof.slug}|iphone:{iid}"

                    writer.write(scope, "mean", mean_w)
                    writer.write(scope, "median", med)
                    writer.write(scope, "std", st)
                    writer.write(scope, "dispersion", disp)
                    writer.write(scope, "count", float(n))

                    combo_debug["case2_shopcohort_iphone"] += 1
                    if len(combo_debug["samples"]) < 5:
                        combo_debug["samples"].append({
                            "case": 2,
                            "scope": scope,
                            "n": n,
                            "mean_w": round(mean_w, 4),
                            "age_p50": (round(_quantile(sorted(ages), 0.5), 2) if ages else None),
                        })
        else:
            combo_debug["skipped"].append("case2: no ShopWeightProfile defined")

        # === CASE 3: å„åº— Ã— ç»„åˆ iPhoneï¼ˆæœºå‹æƒ Ã— æ—¶æ•ˆæƒï¼‰ ===
        for sid in shops_seen:
            for coh in cohorts:
                iw = cmembers.get(coh.id, {})
                if not iw:
                    continue
                vals, ages = [], []
                wnum = wden = 0.0
                for iid, w_phone in iw.items():
                    pair = data_by_si.get((int(sid), int(iid)))
                    if not pair:
                        continue
                    v, t = pair
                    w_rec, age = recency_weight(t, bucket_end)
                    if w_rec <= 0.0:
                        continue
                    w = float(w_phone) * w_rec
                    vals.append(v)
                    if age is not None:
                        ages.append(age)
                    wnum += w * v
                    wden += w

                if not vals:
                    continue

                m_unw, med, st, disp, n = _stats(vals)
                mean_w = (wnum / wden) if wden > 0 else m_unw
                scope = f"shop:{sid}|cohort:{coh.slug}"

                writer.write(scope, "mean", mean_w)
                writer.write(scope, "median", med)
                writer.write(scope, "std", st)
                writer.write(scope, "dispersion", disp)
                writer.write(scope, "count", float(n))

                combo_debug["case3_shop_cohortiphone"] += 1
                if len(combo_debug["samples"]) < 5:
                    combo_debug["samples"].append({
                        "case": 3,
                        "scope": scope,
                        "n": n,
                        "mean_w": round(mean_w, 4),
                        "age_p50": (round(_quantile(sorted(ages), 0.5), 2) if ages else None),
                    })

        # === CASE 4: ç»„åˆåº— Ã— ç»„åˆ iPhoneï¼ˆåº—æƒ Ã— æœºå‹æƒ Ã— æ—¶æ•ˆæƒï¼‰ ===
        if has_shop_profile:
            for prof in profiles:
                sw = prof_items.get(prof.id, {})
                if not sw:
                    continue
                shops_in = set(sw.keys()) & set(shops_seen)
                if not shops_in:
                    continue

                for coh in cohorts:
                    iw = cmembers.get(coh.id, {})
                    if not iw:
                        continue
                    vals, ages = [], []
                    wnum = wden = 0.0
                    for sid, w_shop in sw.items():
                        if int(sid) not in shops_in:
                            continue
                        for iid, w_phone in iw.items():
                            pair = data_by_si.get((int(sid), int(iid)))
                            if not pair:
                                continue
                            v, t = pair
                            w_rec, age = recency_weight(t, bucket_end)
                            if w_rec <= 0.0:
                                continue
                            w = float(w_shop) * float(w_phone) * w_rec
                            vals.append(v)
                            if age is not None:
                                ages.append(age)
                            wnum += w * v
                            wden += w

                    if not vals:
                        continue

                    m_unw, med, st, disp, n = _stats(vals)
                    mean_w = (wnum / wden) if wden > 0 else m_unw
                    scope = f"shopcohort:{prof.slug}|cohort:{coh.slug}"

                    writer.write(scope, "mean", mean_w)
                    writer.write(scope, "median", med)
                    writer.write(scope, "std", st)
                    writer.write(scope, "dispersion", disp)
                    writer.write(scope, "count", float(n))

                    combo_debug["case4_shopcohort_cohortiphone"] += 1
                    if len(combo_debug["samples"]) < 5:
                        combo_debug["samples"].append({
                            "case": 4,
                            "scope": scope,
                            "n": n,
                            "mean_w": round(mean_w, 4),
                            "age_p50": (round(_quantile(sorted(ages), 0.5), 2) if ages else None),
                        })
        else:
            combo_debug["skipped"].append("case4: no ShopWeightProfile defined")

        # æ±‡æ€»æ—¥å¿—è¾“å‡º
        total_features = (
            combo_debug["case1_shop_iphone"] +
            combo_debug["case2_shopcohort_iphone"] +
            combo_debug["case3_shop_cohortiphone"] +
            combo_debug["case4_shopcohort_cohortiphone"]
        )
        logger.info(
            f"  âœï¸  [ç‰¹å¾å†™å…¥] "
            f"Case1(shopÃ—iphone): {combo_debug['case1_shop_iphone']}, "
            f"Case2(shopcohortÃ—iphone): {combo_debug['case2_shopcohort_iphone']}, "
            f"Case3(shopÃ—cohort): {combo_debug['case3_shop_cohortiphone']}, "
            f"Case4(shopcohortÃ—cohort): {combo_debug['case4_shopcohort_cohortiphone']} | "
            f"æ€»è®¡: {total_features} ä¸ªç»„åˆ"
        )

        try:
            notify_progress_all(data={
                "type": "feature_snapshot_update",
                "bucket": ts_iso,
                "summary": {
                    "case1_shop_iphone": combo_debug["case1_shop_iphone"],
                    "case2_shopcohort_iphone": combo_debug["case2_shopcohort_iphone"],
                    "case3_shop_cohortiphone": combo_debug["case3_shop_cohortiphone"],
                    "case4_shopcohort_cohortiphone": combo_debug["case4_shopcohort_cohortiphone"],
                    "skipped": combo_debug["skipped"],
                    "agg": agg_ctx,
                },
                "samples": combo_debug["samples"],
            })
        except Exception:
            pass

    except Exception as e:
        try:
            notify_progress_all(data={
                "type": "feature_snapshot_error",
                "bucket": ts_iso,
                "error": repr(e),
                "agg": agg_ctx,
            })
        except Exception:
            pass

def _agg_time_series_features(
    *,
    ts_iso: str,
    anchor_bucket,
    ob_bucket,
    is_final_bar: bool,
    ob_has_iphone: bool,
    writer,
) -> Dict[str, float]:
    """
    4) æ—¶é—´åºåˆ—æ´¾ç”ŸæŒ‡æ ‡ï¼šSMA / WMA / EMAï¼ˆæŒ‰ FeatureSpecï¼‰

    è¿”å›:
        base_now: scope -> å½“å‰ x_t åŸºå€¼ï¼ˆç»™ Bollinger å¤ç”¨ï¼‰
    """
    from django.apps import apps
    from typing import Dict
    from AppleStockChecker.models import FeatureSnapshot, OverallBar, CohortBar

    FeatureSpec = apps.get_model("AppleStockChecker", "FeatureSpec")

    timefeat_debug = {"bucket": ts_iso, "computed": 0, "skipped": [], "samples": []}
    base_now: Dict[str, float] = {}

    try:
        # å–å‡ºæ‰€æœ‰æ¿€æ´»çš„æ—¶åºç±»æŒ‡æ ‡
        specs = list(
            FeatureSpec.objects
            .filter(active=True, family__in=["sma", "ema", "wma"])
            .values("slug", "family", "base_name", "params", "version")
        )

        # â€”â€” æ”¶é›†"å½“å‰é”šç‚¹"çš„åŸºå€¼ x_tï¼šscope -> x_t â€”â€” #
        # 4.a å››ç±»ç»„åˆï¼ˆFeatureSnapshot.mean@anchor_bucketï¼‰
        for row in (
            FeatureSnapshot.objects
            .filter(bucket=anchor_bucket, name="mean")
            .values("scope", "value")
        ):
            if row["value"] is not None:
                base_now[row["scope"]] = float(row["value"])

        # ===== å·²ç¦ç”¨ï¼šä» OverallBar/CohortBar æ”¶é›†åŸºå€¼ =====
        # åŸå› ï¼šå·²åœç”¨ OverallBar/CohortBar è®¡ç®—
        # å¦‚æœéœ€è¦ scope="overall:iphone:*" æˆ– "cohort:*" çš„æ—¶é—´åºåˆ—æŒ‡æ ‡ï¼Œ
        # è¯·å…ˆæ¢å¤ _run_aggregation ä¸­çš„ OverallBar/CohortBar è®¡ç®—

        # # 4.b OverallBar.mean -> overall:iphone:<id>ï¼ˆ@ob_bucketï¼‰
        # if ob_has_iphone:
        #     for row in (
        #         OverallBar.objects
        #         .filter(bucket=ob_bucket)
        #         .values("iphone_id", "mean")
        #     ):
        #         if row["mean"] is not None:
        #             base_now[f"overall:iphone:{row['iphone_id']}"] = float(row["mean"])

        # # 4.c CohortBar.mean -> cohort:<slug>ï¼ˆ@ob_bucketï¼‰
        # for row in (
        #     CohortBar.objects
        #     .filter(bucket=ob_bucket)
        #     .select_related("cohort")
        #     .values("cohort__slug", "mean")
        # ):
        #     if row["mean"] is not None and row["cohort__slug"]:
        #         base_now[f"cohort:{row['cohort__slug']}"] = float(row["mean"])

        # â€”â€” å·¥å…·ï¼šå›å†™æ´¾ç”Ÿå€¼ï¼ˆåªç»™ SMA ç”¨ï¼ŒEMA/WMA èµ° writerï¼‰ â€”â€” #
        def upsert_feat(scope: str, name: str, version: str, value: float):
            safe_upsert_feature_snapshot(
                bucket=anchor_bucket,
                scope=scope,
                name=name,
                version=version,
                value=value,
                is_final=is_final_bar,
            )

        # â€”â€” é€ spec Ã— scope è®¡ç®— â€”â€” #
        for sp in specs:
            family = (sp["family"] or "").lower()
            spec_slug = sp["slug"]
            base_name = sp.get("base_name") or "mean"
            params = sp.get("params") or {}
            base_version = params.get("base_version", sp.get("version") or "v1")

            # ç»Ÿä¸€çª—å£/æœ€å°æ ·æœ¬æ•°
            W = int(params.get("window", 15))
            min_count = int(params.get("min_count", params.get("min_periods", 1)))
            weights_mode = str(params.get("weights", "linear")).lower()

            def _alpha_from_params(p: dict) -> float:
                if p is None:
                    p = {}
                if p.get("alpha") is not None:
                    a = float(p["alpha"])
                    return max(0.0, min(1.0, a))
                if p.get("window") is not None:
                    W_ = max(1, int(p["window"]))
                    return 2.0 / (W_ + 1.0)
                if p.get("half_life") is not None:
                    hl = float(p["half_life"])
                    return 1.0 - 0.5 ** (1.0 / max(hl, 1e-9))
                return 2.0 / (15.0 + 1.0)  # é»˜è®¤

            for scope, x_t in base_now.items():
                # æ‹‰å–å†å²åŸºå€¼ï¼ˆæ–°->æ—§ï¼‰ï¼Œå†è½¬ä¸ºæ—§->æ–°ï¼Œå¹¶åœ¨æœ«å°¾è¿½åŠ å½“å‰ x_t
                prev_vals = _fetch_prev_base(scope, base_name, base_version, W - 1, anchor_bucket)
                series_old_to_new = list(reversed(prev_vals)) + [float(x_t)]

                # æ ·æœ¬æ•°æ ¡éªŒ
                if len(series_old_to_new) < max(1, min_count):
                    timefeat_debug["skipped"].append(
                        f"{family}:{spec_slug}@{scope}:insufficient({len(series_old_to_new)}<{min_count})"
                    )
                    continue

                try:
                    if family == "ema":
                        alpha = _alpha_from_params(params)
                        val = _ema_from_series(series_old_to_new, alpha)
                        writer.write(scope, "ema", val, version=spec_slug)
                        timefeat_debug["computed"] += 1

                    elif family in ("wma", "wma_linear"):
                        if weights_mode == "linear":
                            val = _wma_linear(series_old_to_new, W)
                        else:
                            # å…¶å®ƒæƒé‡æ¨¡å¼æš‚æœªå®ç° -> åå¤‡ä¸º SMA
                            val = _sma(series_old_to_new, W)
                        if val is None:
                            timefeat_debug["skipped"].append(
                                f"wma:{spec_slug}@{scope}:no_series"
                            )
                            continue

                        writer.write(scope, "wma", val, version=spec_slug)
                        timefeat_debug["computed"] += 1

                    elif family == "sma":
                        val = _sma(series_old_to_new, W)
                        if val is None:
                            timefeat_debug["skipped"].append(
                                f"sma:{spec_slug}@{scope}:no_series"
                            )
                            continue

                        upsert_feat(scope, "sma", spec_slug, val)
                        timefeat_debug["computed"] += 1

                    if len(timefeat_debug["samples"]) < 6:
                        timefeat_debug["samples"].append({
                            "scope": scope,
                            "family": family,
                            "spec": spec_slug,
                            "W": W,
                            "x": round(float(x_t), 2),
                            "y": round(float(val), 2),
                        })
                except Exception as _e:
                    timefeat_debug["skipped"].append(
                        f"{family}:{spec_slug}@{scope}:{repr(_e)}"
                    )

        try:
            notify_progress_all(data={
                "type": "feature_time_series_update",
                "bucket": ts_iso,
                "summary": {k: v for k, v in timefeat_debug.items() if k != "samples"},
                "samples": timefeat_debug["samples"],
            })
        except Exception:
            pass

    except Exception as e:
        try:
            notify_progress_all(data={
                "type": "feature_time_series_error",
                "bucket": ts_iso,
                "error": repr(e),
            })
        except Exception:
            pass

    return base_now


def _agg_bollinger_bands(
    *,
    ts_iso: str,
    anchor_bucket,
    is_final_bar: bool,
    base_now: Dict[str, float],
    writer,
):
    """
    5) Bollinger Bandsï¼ˆæŒ‰ FeatureSpecï¼Œæ”¯æŒ center_mode="sma"/"ema"/"sma60"...ï¼‰
    """
    from django.apps import apps

    FeatureSpec = apps.get_model("AppleStockChecker", "FeatureSpec")

    boll_debug = {"bucket": ts_iso, "computed": 0, "skipped": [], "samples": []}

    try:
        specs_boll = list(
            FeatureSpec.objects
            .filter(active=True, family__in=["boll", "bollinger"])
            .values("slug", "base_name", "params", "version")
        )

        if not specs_boll:
            boll_debug["skipped"].append("no_active_bollinger_spec")
        else:
            def _parse_center_mode(params: dict, default_W: int):
                cm = str(params.get("center_mode", "sma")).lower()
                import re
                m = re.match(r"^(sma|ema)(\d+)?$", cm)
                if m:
                    mode = m.group(1)
                    w = int(m.group(2)) if m.group(2) else default_W
                    return mode, w
                # å…¶å®ƒå†™æ³•é€€åŒ–åˆ° sma
                return "sma", default_W

            for sp in specs_boll:
                spec_slug = sp["slug"]
                base_name = sp.get("base_name") or "mean"
                params = sp.get("params") or {}
                base_version = params.get("base_version", sp.get("version") or "v1")

                W = max(1, int(params.get("window", 20)))
                k = float(params.get("k", 2.0))
                min_periods = int(params.get("min_periods", W))
                center_mode, center_W = _parse_center_mode(params, W)

                for scope, x_t in base_now.items():
                    prev_vals = _fetch_prev_base(scope, base_name, base_version, W - 1, anchor_bucket)
                    series_old_to_new = list(reversed(prev_vals)) + [float(x_t)]
                    if len(series_old_to_new) < max(1, min_periods):
                        boll_debug["skipped"].append(
                            f"{spec_slug}@{scope}:insufficient({len(series_old_to_new)}<{min_periods})"
                        )
                        continue

                    # ä¸­è½¨
                    if center_mode == "ema":
                        alpha = 2.0 / (center_W + 1.0)
                        mid = _ema_from_series(series_old_to_new, alpha)
                    else:
                        mid = _sma(series_old_to_new, center_W)

                    std = _pop_std(series_old_to_new) or 0.0
                    up = mid + k * std
                    low = mid - k * std
                    width = up - low

                    rows = [
                        FeatureRecord(
                            bucket=anchor_bucket,
                            scope=scope,
                            name="boll_mid",
                            version=spec_slug,
                            value=mid,
                            is_final=is_final_bar,
                        ),
                        FeatureRecord(
                            bucket=anchor_bucket,
                            scope=scope,
                            name="boll_up",
                            version=spec_slug,
                            value=up,
                            is_final=is_final_bar,
                        ),
                        FeatureRecord(
                            bucket=anchor_bucket,
                            scope=scope,
                            name="boll_low",
                            version=spec_slug,
                            value=low,
                            is_final=is_final_bar,
                        ),
                        FeatureRecord(
                            bucket=anchor_bucket,
                            scope=scope,
                            name="boll_width",
                            version=spec_slug,
                            value=width,
                            is_final=is_final_bar,
                        ),
                    ]
                    writer.write_many(rows)

                    boll_debug["computed"] += 1
                    if len(boll_debug["samples"]) < 6:
                        boll_debug["samples"].append({
                            "scope": scope,
                            "spec": spec_slug,
                            "W": W,
                            "center": f"{center_mode}{center_W}",
                            "mid": round(mid, 2),
                            "up": round(up, 2),
                            "low": round(low, 2),
                        })

        try:
            notify_progress_all(data={
                "type": "feature_boll_update",
                "bucket": ts_iso,
                "summary": {k: v for k, v in boll_debug.items() if k != "samples"},
                "samples": boll_debug["samples"],
            })
        except Exception:
            pass

    except Exception as e:
        try:
            notify_progress_all(data={
                "type": "feature_boll_error",
                "bucket": ts_iso,
                "error": repr(e),
            })
        except Exception:
            pass


def _agg_market_log_premium(
    *,
    ts_iso: str,
    anchor_bucket,
    is_final_bar: bool,
    writer,
):
    """
    6) å¸‚åœº log æº¢ä»·ï¼ˆMarket Log Premiumï¼‰
    å…¬å¼: b_{k,t}^{wma_{tm}} = log(QÌ„^{wma_{tm}}_{k,t} / P^{official}_k)

    å…¶ä¸­ï¼š
    - QÌ„^{wma_{tm}}_{k,t}: FeatureSnapshot ä¸­ scope="shopcohort:full_store|iphone:{k}", name="wma", version="wma{tm}m" çš„å€¼
    - P^{official}_k: iPhone ID=k çš„å®˜æ–¹å‘å¸ƒä»·æ ¼ï¼ˆä» settings.IPHONE_OFFICIAL_PRICES è·å–ï¼‰
    - ç»“æœå­˜å‚¨ä¸ºï¼šname="logb", version=å¯¹åº”çš„ wma ç‰ˆæœ¬ï¼ˆå¦‚ "wma120m"ï¼‰
    """
    import re
    import math
    from django.apps import apps
    from django.conf import settings

    FeatureSnapshot = apps.get_model("AppleStockChecker", "FeatureSnapshot")

    logb_debug = {"bucket": ts_iso, "computed": 0, "skipped": [], "samples": []}

    try:
        # è·å–å®˜æ–¹ä»·æ ¼é…ç½®
        official_prices = getattr(settings, "IPHONE_OFFICIAL_PRICES", {})
        if not official_prices:
            logb_debug["skipped"].append("no_official_prices_in_settings")
            try:
                notify_progress_all(data={
                    "type": "feature_logb_warning",
                    "bucket": ts_iso,
                    "message": "IPHONE_OFFICIAL_PRICES not configured in settings",
                })
            except Exception:
                pass
            return

        # æŸ¥è¯¢å½“å‰ bucket ä¸­æ‰€æœ‰ name="wma" çš„ FeatureSnapshot è®°å½•
        # åªå…³æ³¨ scope åŒ…å« "shopcohort:full_store" çš„è®°å½•
        wma_records = list(
            FeatureSnapshot.objects
            .filter(
                bucket=anchor_bucket,
                name="wma",
                scope__startswith="shopcohort:full_store|iphone:"
            )
            .values("scope", "version", "value")
        )

        if not wma_records:
            logb_debug["skipped"].append("no_wma_records_for_full_store")
        else:
            # æ­£åˆ™åŒ¹é… version æ ¼å¼ï¼šwma[0-9]+m
            wma_version_pattern = re.compile(r"^wma\d+m$")

            for rec in wma_records:
                scope = rec["scope"]
                version = rec["version"]
                wma_value = rec["value"]

                # æ£€æŸ¥ version æ ¼å¼
                if not wma_version_pattern.match(version):
                    logb_debug["skipped"].append(f"{scope}@{version}:invalid_version_format")
                    continue

                # ä» scope ä¸­æå– iPhone ID
                # scope æ ¼å¼: "shopcohort:full_store|iphone:{iphone_id}"
                try:
                    iphone_id = int(scope.split("|iphone:")[-1])
                except (ValueError, IndexError):
                    logb_debug["skipped"].append(f"{scope}@{version}:invalid_scope_format")
                    continue

                # è·å–å®˜æ–¹ä»·æ ¼
                official_price = official_prices.get(iphone_id)
                if official_price is None:
                    logb_debug["skipped"].append(f"{scope}@{version}:no_official_price_for_iphone_{iphone_id}")
                    continue

                if official_price <= 0:
                    logb_debug["skipped"].append(f"{scope}@{version}:invalid_official_price_{official_price}")
                    continue

                # æ£€æŸ¥ wma_value æœ‰æ•ˆæ€§
                if wma_value is None or wma_value <= 0:
                    logb_debug["skipped"].append(f"{scope}@{version}:invalid_wma_value_{wma_value}")
                    continue

                # è®¡ç®— log æº¢ä»·ï¼šlog(wma_value / official_price)
                try:
                    logb_value = math.log(wma_value / official_price)
                except (ValueError, ZeroDivisionError) as e:
                    logb_debug["skipped"].append(f"{scope}@{version}:math_error_{repr(e)}")
                    continue

                # å†™å…¥æ–°çš„ FeatureSnapshot
                logb_record = FeatureRecord(
                    bucket=anchor_bucket,
                    scope=scope,
                    name="logb",
                    version=version,  # ä½¿ç”¨ä¸ wma ç›¸åŒçš„ version
                    value=logb_value,
                    is_final=is_final_bar,
                )
                writer.write_many([logb_record])

                logb_debug["computed"] += 1
                if len(logb_debug["samples"]) < 6:
                    logb_debug["samples"].append({
                        "scope": scope,
                        "version": version,
                        "iphone_id": iphone_id,
                        "wma": round(wma_value, 2),
                        "official": official_price,
                        "logb": round(logb_value, 4),
                    })

        # å‘é€è¿›åº¦é€šçŸ¥
        try:
            notify_progress_all(data={
                "type": "feature_logb_update",
                "bucket": ts_iso,
                "summary": {k: v for k, v in logb_debug.items() if k != "samples"},
                "samples": logb_debug["samples"],
            })
        except Exception:
            pass

    except Exception as e:
        try:
            notify_progress_all(data={
                "type": "feature_logb_error",
                "bucket": ts_iso,
                "error": repr(e),
            })
        except Exception:
            pass


# === å¯è°ƒå‚æ•°ï¼ˆæ ¹æ®ä½ ä»¬å‰åç«¯é“¾è·¯å®¹é‡è°ƒèŠ‚ï¼‰ ===
MAX_BUCKET_ERROR_SAMPLES = 50  # å•æ¡¶ä¿ç•™çš„ error æ˜ç»†æ¡æ•°ä¸Šé™
MAX_BUCKET_CHART_POINTS = 3000  # å•æ¡¶æ‰“åŒ…ç»™å›è°ƒèšåˆç”¨çš„ chart point ä¸Šé™
MAX_PUSH_POINTS = 60000  # æœ¬æ¬¡å¹¿æ’­ç»™å‰ç«¯çš„ point æ€»ä¸Šé™ï¼ˆè¶…è¿‡åˆ™è£å‰ªåˆ°æœ€è¿‘ N æ¡ï¼‰

# åºŸå¼ƒï¼šå›ºå®šçš„ä»·æ ¼é˜ˆå€¼ï¼ˆä¿ç•™ç”¨äºåå¤‡ï¼‰
PRICE_MIN = 100000
PRICE_MAX = 350000

# åŠ¨æ€ä»·æ ¼åŒºé—´é…ç½®
PRICE_LOOKBACK_MINUTES = 30  # å‘å‰æŸ¥è¯¢å¤šå°‘åˆ†é’Ÿçš„æ•°æ®ä½œä¸ºå‚è€ƒ
PRICE_TOLERANCE_RATIO = 0.10  # å®¹å·®æ¯”ä¾‹ï¼šÂ±50%
PRICE_MIN_SAMPLES = 3  # è®¡ç®—å‚è€ƒä»·æ ¼æ‰€éœ€çš„æœ€å°‘æ ·æœ¬æ•°
PRICE_FALLBACK_MIN = 100000  # æ•°æ®ä¸è¶³æ—¶çš„åå¤‡æœ€å°å€¼
PRICE_FALLBACK_MAX = 350000  # æ•°æ®ä¸è¶³æ—¶çš„åå¤‡æœ€å¤§å€¼


def get_dynamic_price_range(
    iphone_id: int,
    reference_time,
    lookback_minutes: int = PRICE_LOOKBACK_MINUTES,
    tolerance_ratio: float = PRICE_TOLERANCE_RATIO,
    min_samples: int = PRICE_MIN_SAMPLES,
) -> tuple[float, float]:
    """
    æ ¹æ®æŒ‡å®š iPhone å‹å·åœ¨å‚è€ƒæ—¶é—´ç‚¹å‰ N åˆ†é’Ÿå†…çš„å†å²ä»·æ ¼ï¼Œ
    åŠ¨æ€è®¡ç®—è¯¥å‹å·çš„åˆç†ä»·æ ¼åŒºé—´ã€‚

    é€»è¾‘ï¼š
    1. æŸ¥è¯¢è¯¥ iphone_id åœ¨ [reference_time - lookback_minutes, reference_time) æ—¶é—´çª—å£å†…
       æ‰€æœ‰ä¸åŒåº—é“ºçš„æœ€æ–°ä»·æ ¼è®°å½•
    2. è®¡ç®—è¿™äº›ä»·æ ¼çš„å¹³å‡å€¼ä½œä¸ºå‚è€ƒä»·æ ¼
    3. åŸºäºå‚è€ƒä»·æ ¼å’Œå®¹å·®æ¯”ä¾‹ï¼Œè®¡ç®—ä»·æ ¼åŒºé—´ï¼š
       - price_min = reference_price * (1 - tolerance_ratio)
       - price_max = reference_price * (1 + tolerance_ratio)
    4. å¦‚æœæ ·æœ¬æ•°ä¸è¶³ï¼Œè¿”å›åå¤‡çš„å›ºå®šåŒºé—´

    å‚æ•°ï¼š
        iphone_id: iPhone å‹å· ID
        reference_time: å‚è€ƒæ—¶é—´ç‚¹ï¼ˆdatetime å¯¹è±¡ï¼‰
        lookback_minutes: å‘å‰æŸ¥è¯¢çš„æ—¶é—´çª—å£ï¼ˆåˆ†é’Ÿï¼‰
        tolerance_ratio: ä»·æ ¼å®¹å·®æ¯”ä¾‹ï¼ˆ0.5 è¡¨ç¤º Â±50%ï¼‰
        min_samples: è®¡ç®—å‚è€ƒä»·æ ¼æ‰€éœ€çš„æœ€å°‘æ ·æœ¬æ•°

    è¿”å›ï¼š
        (price_min, price_max): åŠ¨æ€è®¡ç®—çš„ä»·æ ¼åŒºé—´
    """
    from django.db.models import Avg, Count
    from AppleStockChecker.models import PurchasingShopTimeAnalysis

    # è®¡ç®—æŸ¥è¯¢æ—¶é—´çª—å£
    start_time = reference_time - timedelta(minutes=lookback_minutes)

    # æŸ¥è¯¢è¯¥æ—¶é—´çª—å£å†…ä¸åŒåº—é“ºçš„æœ€æ–°ä»·æ ¼ï¼ˆæ¯ä¸ªåº—é“ºå–æœ€æ–°ä¸€æ¡ï¼‰
    # ä½¿ç”¨å­æŸ¥è¯¢è·å–æ¯ä¸ªåº—é“ºçš„æœ€æ–°è®°å½•
    subquery = (
        PurchasingShopTimeAnalysis.objects
        .filter(
            iphone_id=iphone_id,
            Timestamp_Time__gte=start_time,
            Timestamp_Time__lt=reference_time,
            New_Product_Price__isnull=False,
        )
        .order_by("shop_id", "-Timestamp_Time")
        .distinct("shop_id")
        .values_list("New_Product_Price", flat=True)
    )

    prices = list(subquery)

    # å¦‚æœæ ·æœ¬æ•°ä¸è¶³ï¼Œè¿”å›åå¤‡åŒºé—´
    if len(prices) < min_samples:
        # logger.warning(
        #     f"åŠ¨æ€ä»·æ ¼åŒºé—´: iphone_id={iphone_id}, æ ·æœ¬æ•°ä¸è¶³({len(prices)}/{min_samples}), "
        #     f"ä½¿ç”¨åå¤‡åŒºé—´ [{PRICE_FALLBACK_MIN}, {PRICE_FALLBACK_MAX}]"
        # )
        return PRICE_FALLBACK_MIN, PRICE_FALLBACK_MAX

    # è®¡ç®—å¹³å‡ä»·æ ¼ä½œä¸ºå‚è€ƒä»·æ ¼
    reference_price = sum(prices) / len(prices)

    # è®¡ç®—åŠ¨æ€åŒºé—´
    price_min = reference_price * (1 - tolerance_ratio)
    price_max = reference_price * (1 + tolerance_ratio)

    # ç¡®ä¿åŒºé—´ä¸ä¼šè¿‡å°ï¼ˆè‡³å°‘ä¿ç•™ä¸€å®šçš„èŒƒå›´ï¼‰
    min_range = reference_price * 0.2  # è‡³å°‘ Â±10%
    if price_max - price_min < min_range:
        price_min = reference_price * 0.9
        price_max = reference_price * 1.1

    # logger.info(
    #     f"åŠ¨æ€ä»·æ ¼åŒºé—´: iphone_id={iphone_id}, æ ·æœ¬æ•°={len(prices)}, "
    #     f"å‚è€ƒä»·æ ¼={reference_price:.0f}, åŒºé—´=[{price_min:.0f}, {price_max:.0f}]"
    # )

    return price_min, price_max


def is_price_valid(
    price: float,
    iphone_id: int,
    reference_time,
    use_dynamic_range: bool = True,
) -> bool:
    """
    æ£€æŸ¥ä»·æ ¼æ˜¯å¦åœ¨åˆç†åŒºé—´å†…ã€‚

    å‚æ•°ï¼š
        price: å¾…æ£€æŸ¥çš„ä»·æ ¼
        iphone_id: iPhone å‹å· ID
        reference_time: å‚è€ƒæ—¶é—´ç‚¹
        use_dynamic_range: æ˜¯å¦ä½¿ç”¨åŠ¨æ€ä»·æ ¼åŒºé—´ï¼ˆé»˜è®¤ Trueï¼‰

    è¿”å›ï¼š
        bool: ä»·æ ¼æ˜¯å¦æœ‰æ•ˆ
    """
    if use_dynamic_range:
        price_min, price_max = get_dynamic_price_range(iphone_id, reference_time)
    else:
        # ä½¿ç”¨å›ºå®šåŒºé—´
        price_min, price_max = PRICE_MIN, PRICE_MAX

    return price_min <= price <= price_max


# -----------------------------------------------------
# --------------------------------------------------------
# -----------------------------------------------------------
# ---------------------------------------------------------------
# -----------------------------------------------------------
# --------------------------------------------------------
# -----------------------------------------------------
# -----------------------------------------------
# å­ä»»åŠ¡ï¼šå¤„ç†"åˆ†é’Ÿæ¡¶"å¹¶è¿”å›æ¡¶çº§æ‘˜è¦ + å›¾è¡¨å¢é‡
# -----------------------------------------------
TASK_VER_PSTA = 3


def _process_minute_rows(*, ts_iso: str, ts_dt, rows, job_id: str):
    """
    æ¨¡å—ä¸€ï¼šåˆ†é’Ÿå¯¹é½æ•°æ®å†™å…¥ PurchasingShopTimeAnalysisï¼Œå¹¶æ”¶é›† chart_pointsã€‚

    è¿”å›:
        ok, failed, err_counter, errors, chart_points
    """
    from django.utils import timezone  # å¦‚æœä¸éœ€è¦å¯ä»¥å»æ‰
    from django.core.exceptions import ObjectDoesNotExist, ValidationError
    from AppleStockChecker.models import (
        PurchasingShopTimeAnalysis, SecondHandShop, Iphone,
    )

    ok = 0
    failed = 0
    errors = []
    err_counter = Counter()

    ts_tz = _tz_offset_str(ts_dt)
    orig_tz = "+09:00"
    chart_points = []

    # é¢„è®¡ç®—æ‰€æœ‰ iphone_id çš„åŠ¨æ€ä»·æ ¼åŒºé—´ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼Œé¿å…é‡å¤æŸ¥è¯¢ï¼‰
    unique_iphone_ids = {r.get("iphone_id") for r in rows if r.get("iphone_id")}
    price_ranges_cache = {}
    for iphone_id in unique_iphone_ids:
        try:
            price_ranges_cache[iphone_id] = get_dynamic_price_range(iphone_id, ts_dt)
        except Exception as e:
            logger.warning(f"è®¡ç®—åŠ¨æ€ä»·æ ¼åŒºé—´å¤±è´¥: iphone_id={iphone_id}, error={e}, ä½¿ç”¨åå¤‡åŒºé—´")
            price_ranges_cache[iphone_id] = (PRICE_FALLBACK_MIN, PRICE_FALLBACK_MAX)

    for r in rows:
        try:
            # è½»é‡æ ¡éªŒ
            shop_id = r.get("shop_id")
            iphone_id = r.get("iphone_id")
            if not shop_id or not iphone_id:
                raise ValueError("missing shop_id/iphone_id")

            # å¤–é”®å­˜åœ¨æ€§
            SecondHandShop.objects.only("id").get(pk=shop_id)
            Iphone.objects.only("id").get(pk=iphone_id)

            rec_dt = _to_aware(r.get("recorded_at"))
            new_price = r.get("price_new") or r.get("New_Product_Price")
            if new_price is None:
                raise ValueError("missing New_Product_Price")

            try:
                price = int(new_price)
            except (TypeError, ValueError):
                raise ValueError(f"bad New_Product_Price: {new_price!r}")

            # ä½¿ç”¨åŠ¨æ€ä»·æ ¼åŒºé—´è¿‡æ»¤ï¼ˆæ›¿æ¢å›ºå®šé˜ˆå€¼ï¼‰
            price_min, price_max = price_ranges_cache.get(iphone_id, (PRICE_FALLBACK_MIN, PRICE_FALLBACK_MAX))
            if not (price_min <= price <= price_max):
                logger.debug(
                    f"ä»·æ ¼è¶…å‡ºåŠ¨æ€åŒºé—´: shop_id={shop_id}, iphone_id={iphone_id}, "
                    f"price={price}, åŒºé—´=[{price_min:.0f}, {price_max:.0f}]"
                )
                continue

            align_diff = int((rec_dt - ts_dt).total_seconds())

            from django.db import transaction
            from django.db.utils import IntegrityError as DjangoIntegrityError

            with transaction.atomic():
                inst = (
                    PurchasingShopTimeAnalysis.objects
                    .select_for_update()
                    .filter(
                        shop_id=shop_id,
                        iphone_id=iphone_id,
                        Timestamp_Time=ts_dt,
                    )
                    .first()
                )

                if inst:
                    inst.Job_ID = job_id
                    inst.Original_Record_Time_Zone = orig_tz
                    inst.Timestamp_Time_Zone = ts_tz
                    inst.Record_Time = rec_dt
                    inst.Alignment_Time_Difference = align_diff
                    inst.New_Product_Price = int(new_price)
                    inst.Update_Count = (inst.Update_Count or 0) + 1
                    inst.save()
                else:
                    inst = PurchasingShopTimeAnalysis.objects.create(
                        Batch_ID=None,
                        Job_ID=job_id,
                        Original_Record_Time_Zone=orig_tz,
                        Timestamp_Time_Zone=ts_tz,
                        Record_Time=rec_dt,
                        Timestamp_Time=ts_dt,
                        Alignment_Time_Difference=align_diff,
                        Update_Count=0,
                        shop_id=shop_id,
                        iphone_id=iphone_id,
                        New_Product_Price=int(new_price),
                    )

            ok += 1

            # æ”¶é›†å›¾è¡¨å¢é‡ï¼ˆå‰ç«¯å»é‡ï¼‰
            if len(chart_points) < MAX_BUCKET_CHART_POINTS:
                chart_points.append({
                    "id": inst.pk,
                    "t": ts_iso,
                    "iphone_id": iphone_id,
                    "shop_id": shop_id,
                    "price": int(new_price),
                    "recorded_at": rec_dt.isoformat(),
                })

        except (ObjectDoesNotExist, ValidationError, IntegrityError, TypeError, ValueError) as e:
            failed += 1
            err_counter[e.__class__.__name__] += 1
            if len(errors) < MAX_BUCKET_ERROR_SAMPLES:
                errors.append({
                    "exc": e.__class__.__name__,
                    "msg": str(e),
                    "item": {
                        "shop_id": r.get("shop_id"),
                        "iphone_id": r.get("iphone_id"),
                        "recorded_at": r.get("recorded_at"),
                        "New_Product_Price": r.get("price_new") or r.get("New_Product_Price"),
                    },
                })
        except Exception as e:
            failed += 1
            err_counter[e.__class__.__name__] += 1
            if len(errors) < MAX_BUCKET_ERROR_SAMPLES:
                errors.append({
                    "exc": e.__class__.__name__,
                    "msg": str(e),
                    "item": {
                        "shop_id": r.get("shop_id"),
                        "iphone_id": r.get("iphone_id"),
                        "recorded_at": r.get("recorded_at"),
                    },
                })

    return ok, failed, err_counter, errors, chart_points


def _run_aggregation(
    *,
    ts_iso: str,
    ts_dt,
    rows: List[Dict[str, Any]],
    agg_start_iso: Optional[str],
    agg_minutes: int,
):
    """
    èšåˆè°ƒåº¦å™¨ï¼šæŒ‰é¡ºåºè°ƒç”¨ 6 ä¸ªå­æ­¥éª¤ï¼š
    1) OverallBar
    2) CohortBar
    3) FeatureSnapshot å››ç±»ç»„åˆ
    4) æ—¶é—´åºåˆ—
    5) Bollinger Bands
    6) Market Log Premiumï¼ˆå¸‚åœº log æº¢ä»·ï¼‰
    """
    logger.info(
        f"ğŸ”„ [FeatureSnapshot èšåˆ] è¿›å…¥èšåˆæµç¨‹ | "

    )
    from django.utils import timezone
    from AppleStockChecker.models import OverallBar
    # FeatureWriter / FeatureRecord å·²åœ¨æ¨¡å—é¡¶éƒ¨ä» utils.timestamp_alignment_task å¯¼å…¥

    WATERMARK_MINUTES = 5
    now = timezone.now()
    is_final_bar = ts_dt <= (now - timezone.timedelta(minutes=WATERMARK_MINUTES))

    # --- èšåˆçª—å£ï¼ˆ1åˆ†é’Ÿï¼šç”¨ ts_dtï¼›>1åˆ†é’Ÿï¼šç”¨ [bucket_start, bucket_end) çª—å£ï¼‰ ---
    bucket_start = _to_aware(agg_start_iso) if (agg_minutes and agg_start_iso) else ts_dt
    bucket_end = bucket_start + timezone.timedelta(minutes=agg_minutes or 1)
    use_window = (agg_minutes or 1) > 1

    # ========== FeatureSnapshot èšåˆå¼€å§‹ ==========
    logger.info(
        f"ğŸ”„ [FeatureSnapshot èšåˆ] å¼€å§‹è®¡ç®— | "
        f"æ—¶é—´ç‚¹: {ts_iso} | "
        f"çª—å£: {bucket_start.isoformat()} â†’ {bucket_end.isoformat()} | "
        f"èšåˆæ­¥é•¿: {agg_minutes}åˆ†é’Ÿ | "
        f"æ¨¡å¼: {'çª—å£' if use_window else 'å•åˆ†é’Ÿ'}"
    )
    # =============================================

    # === ç»Ÿä¸€é”šç‚¹ï¼šæ‰€æœ‰ FeatureSnapshot / æ´¾ç”ŸæŒ‡æ ‡çš„ bucketï¼Œéƒ½ç”¨ anchor_bucket ===
    anchor_bucket = bucket_start if use_window else ts_dt
    ob_bucket = anchor_bucket  # OverallBar / CohortBar ä¹Ÿç”¨åŒä¸€é”šç‚¹

    agg_ctx = {
        "do_agg": True,
        "agg_minutes": int(agg_minutes or 1),
        "bucket_start": bucket_start.isoformat(),
        "bucket_end": bucket_end.isoformat(),
    }

    # å†™ feature çš„ç»Ÿä¸€ Writerï¼ˆLWWï¼‰
    writer = FeatureWriter(
        bucket=anchor_bucket,
        default_version="v1",
        is_final=is_final_bar,
        escalate_is_final=False,  # LWWï¼ˆåå†™è¦†ç›–å‰å†™ï¼‰
    )

    # è‡ªåŠ¨æ¢æµ‹ OverallBar æ˜¯å¦å« iphone å¤–é”®ï¼›è‹¥æ²¡æœ‰ï¼Œè·³è¿‡ä»¥å… unique(bucket) å†²çª
    ob_has_iphone = any(
        getattr(f, "name", "") == "iphone"
        for f in OverallBar._meta.get_fields()
    )

    # ===== å·²ç¦ç”¨ï¼šOverallBar å’Œ CohortBar è®¡ç®— =====
    # åŸå› ï¼šä¸»è¦ä½¿ç”¨ FeatureSnapshot å››ç±»ç»„åˆï¼Œæ— éœ€å…¨åº—èšåˆç»Ÿè®¡
    # å¦‚éœ€æ¢å¤ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š

    # # 1) OverallBar
    # _agg_overallbar(
    #     ts_iso=ts_iso,
    #     ts_dt=ts_dt,
    #     rows=rows,
    #     use_window=use_window,
    #     bucket_start=bucket_start,
    #     bucket_end=bucket_end,
    #     is_final_bar=is_final_bar,
    #     agg_ctx=agg_ctx,
    #     ob_has_iphone=ob_has_iphone,
    # )

    # # 2) CohortBar
    # _agg_cohortbar(
    #     ts_iso=ts_iso,
    #     ob_bucket=ob_bucket,
    #     is_final_bar=is_final_bar,
    #     agg_ctx=agg_ctx,
    #     ob_has_iphone=ob_has_iphone,
    # )

    # 3) FeatureSnapshot å››ç±»ç»„åˆ
    _agg_feature_combos(
        ts_iso=ts_iso,
        ts_dt=ts_dt,
        rows=rows,
        bucket_start=bucket_start,
        bucket_end=bucket_end,
        use_window=use_window,
        anchor_bucket=anchor_bucket,
        agg_ctx=agg_ctx,
        is_final_bar=is_final_bar,
        writer=writer,
    )

    # æŸ¥è¯¢åˆšç”Ÿæˆçš„ FeatureSnapshot æ•°æ®é‡
    from AppleStockChecker.models import FeatureSnapshot
    feature_count = FeatureSnapshot.objects.filter(bucket=anchor_bucket).count()
    logger.info(
        f"âœ… [FeatureSnapshot èšåˆ] å®Œæˆ | "
        f"æ—¶é—´ç‚¹: {ts_iso} | "
        f"bucket: {anchor_bucket.isoformat()} | "
        f"ç”Ÿæˆè®°å½•æ•°: {feature_count} æ¡"
    )

    # 4) æ—¶é—´åºåˆ—ï¼ˆè¿”å› base_now ç»™ Bollinger ç”¨ï¼‰
    base_now = _agg_time_series_features(
        ts_iso=ts_iso,
        anchor_bucket=anchor_bucket,
        ob_bucket=ob_bucket,
        is_final_bar=is_final_bar,
        ob_has_iphone=ob_has_iphone,
        writer=writer,
    )

    # 5) Bollinger Bands
    _agg_bollinger_bands(
        ts_iso=ts_iso,
        anchor_bucket=anchor_bucket,
        is_final_bar=is_final_bar,
        base_now=base_now,
        writer=writer,
    )

    # 6) Market Log Premium
    _agg_market_log_premium(
        ts_iso=ts_iso,
        anchor_bucket=anchor_bucket,
        is_final_bar=is_final_bar,
        writer=writer,
    )



@shared_task(name="AppleStockChecker.tasks.psta_process_minute_bucket")
def psta_process_minute_bucket(
    *,
    ts_iso: str,
    rows: list[dict],
    job_id: str,
    agg_minutes: int = 1,  # ä¿ç•™ç”¨äºæ—¥å¿—/è°ƒè¯•
    task_ver: int | None = None,
    **_compat,  # å‘åå…¼å®¹ï¼šæ¥å—å·²åºŸå¼ƒå‚æ•° do_agg, agg_start_iso
) -> dict:
    """
    å­ä»»åŠ¡ï¼šå‚æ•°å®ˆå« + å†™å…¥åˆ†é’Ÿæ•°æ®ï¼ˆv3èµ·èšåˆç§»è‡³ finalizeï¼‰

    v3 å˜æ›´ï¼š
    - ç§»é™¤ do_agg å‚æ•°ï¼ˆèšåˆé€»è¾‘ç§»è‡³ psta_finalize_bucketsï¼‰
    - ç§»é™¤ agg_start_iso å‚æ•°
    - ä¿ç•™ agg_minutes ç”¨äºæ—¥å¿—/è°ƒè¯•
    """
    # æ£€æŸ¥åºŸå¼ƒå‚æ•°å¹¶è®°å½•è­¦å‘Š
    if "do_agg" in _compat:
        logger.warning(f"[psta_process_minute_bucket] do_agg å‚æ•°å·²åºŸå¼ƒ(v3)ï¼Œå°†è¢«å¿½ç•¥")
    if "agg_start_iso" in _compat:
        logger.warning(f"[psta_process_minute_bucket] agg_start_iso å‚æ•°å·²åºŸå¼ƒ(v3)ï¼Œå°†è¢«å¿½ç•¥")

    # ---------- å‚æ•°å®ˆå« ----------
    incoming = dict(
        ts_iso=ts_iso,
        rows=rows,
        job_id=job_id,
        agg_minutes=agg_minutes,
        task_ver=task_ver,
    )

    normalized, meta = guard_params(
        "psta_process_minute_bucket",
        incoming,
        required={"ts_iso": str, "rows": list, "job_id": str},
        optional={
            "agg_minutes": (int, str),
            "task_ver": (int, str, type(None)),
        },
        defaults={"agg_minutes": 1},
        coerce={"agg_minutes": to_int},
        task_ver_field="task_ver",
        expected_ver=TASK_VER_PSTA,
        notify=notify_progress_all,
    )

    # ç”¨å½’ä¸€åŒ–åçš„å€¼è¦†ç›–æœ¬åœ°å˜é‡
    ts_iso = normalized["ts_iso"]
    rows = normalized["rows"] or []
    job_id = normalized["job_id"]
    agg_minutes = normalized.get("agg_minutes", 1)

    # ---------- å†™å…¥åˆ†é’Ÿå¯¹é½æ•°æ® ----------
    ts_dt = _to_aware(ts_iso)

    ok, failed, err_counter, errors, chart_points = _process_minute_rows(
        ts_iso=ts_iso,
        ts_dt=ts_dt,
        rows=rows,
        job_id=job_id,
    )

    if failed:
        try:
            notify_progress_all(data={
                "type": "bucket_errors",
                "ts_iso": ts_iso,
                "total": ok + failed,
                "ok": ok,
                "failed": failed,
                "error_hist": dict(err_counter),
                "sample": errors[:5],
            })
        except Exception:
            pass

    # v3: èšåˆé€»è¾‘å·²ç§»è‡³ psta_finalize_bucketsï¼Œæ­¤å¤„ä¸å†æ‰§è¡Œ

    # ---------- è¿”å›ä»»åŠ¡ç»“æœ ----------
    return {
        "ts_iso": ts_iso,
        "ok": ok,
        "failed": failed,
        "total": ok + failed,
        "error_hist": dict(err_counter),
        "errors": errors[:MAX_BUCKET_ERROR_SAMPLES],
        "chart_points": chart_points,
    }


# è¾…åŠ©ä»»åŠ¡ï¼šç”¨äº chain æ¨¡å¼ä¸‹ç´¯ç§¯ç»“æœ
# -----------------------------------------------


@shared_task(name="AppleStockChecker.tasks.psta_collect_result")
def psta_collect_result(prev_result, current_result=None):
    """
    ç”¨äº chain æ¨¡å¼ä¸‹ç´¯ç§¯æ‰€æœ‰å­ä»»åŠ¡çš„ç»“æœã€‚

    Args:
        prev_result: ä¸Šä¸€ä¸ªä»»åŠ¡ä¼ é€’çš„ç´¯ç§¯ç»“æœåˆ—è¡¨ï¼Œæˆ–å•ä¸ªç»“æœå­—å…¸
        current_result: å½“å‰ä»»åŠ¡çš„ç»“æœï¼ˆç”¨äºé¦–æ¬¡è°ƒç”¨ï¼‰

    Returns:
        ç´¯ç§¯çš„ç»“æœåˆ—è¡¨
    """
    # åˆå§‹åŒ–ç´¯ç§¯åˆ—è¡¨
    if prev_result is None:
        accumulated = []
    elif isinstance(prev_result, list):
        accumulated = prev_result
    else:
        # ç¬¬ä¸€ä¸ªç»“æœæ˜¯å•ä¸ªå­—å…¸
        accumulated = [prev_result]

    # æ·»åŠ å½“å‰ç»“æœ
    if current_result is not None:
        accumulated.append(current_result)

    return accumulated


# -----------------------------------------------
# ç‹¬ç«‹èšåˆä»»åŠ¡ï¼ˆv3æ–°å¢ï¼‰
# -----------------------------------------------


@shared_task(
    name="AppleStockChecker.tasks.psta_aggregate_features",
    queue="psta_aggregation",
    soft_time_limit=240,
    time_limit=360,
)
def psta_aggregate_features(
    *,
    ts_iso: str,
    job_id: str,
    agg_ctx: dict,
    task_ver: int | None = None,
    **_compat,
) -> dict:
    """
    ç‹¬ç«‹èšåˆä»»åŠ¡ï¼ˆv3æ–°å¢ï¼‰

    åœ¨æ‰€æœ‰åˆ†é’Ÿæ¡¶æ•°æ®å†™å…¥å®Œæˆåæ‰§è¡Œç»Ÿè®¡èšåˆã€‚
    ç”± psta_finalize_buckets åœ¨è¾¹ç•Œæ—¶é—´ç‚¹åŒæ­¥è°ƒç”¨ã€‚

    å‚æ•°:
        ts_iso: æ—¶é—´æˆ³ ISO æ ¼å¼
        job_id: ä»»åŠ¡ ID
        agg_ctx: èšåˆä¸Šä¸‹æ–‡ï¼ŒåŒ…å«:
            - agg_minutes: èšåˆæ­¥é•¿
            - agg_mode: èšåˆæ¨¡å¼ (off ç¦ç”¨ï¼Œå…¶ä»– = è¾¹ç•Œæ¨¡å¼)
            - is_boundary: æ˜¯å¦ä¸ºè¾¹ç•Œæ—¶é—´ç‚¹
            - bucket_start: èšåˆçª—å£èµ·å§‹
            - bucket_end: èšåˆçª—å£ç»“æŸ
        task_ver: ä»»åŠ¡ç‰ˆæœ¬å·

    è¿”å›:
        èšåˆç»“æœæˆ–é”™è¯¯ä¿¡æ¯
    """
    agg_result = {
        "ts_iso": ts_iso,
        "job_id": job_id,
        "aggregation_success": False,
        "error": None,
    }

    try:
        # å‚æ•°æ ¡éªŒ
        if not agg_ctx:
            raise ValueError("agg_ctx is required")

        agg_mode = (agg_ctx.get("agg_mode") or "boundary").lower()
        is_boundary = agg_ctx.get("is_boundary", False)
        agg_minutes = int(agg_ctx.get("agg_minutes", 15))
        agg_start_iso = agg_ctx.get("bucket_start")

        # å¦‚æœ agg_mode ä¸º offï¼Œä¸æ‰§è¡Œèšåˆ
        if agg_mode == "off":
            logger.info(f"[psta_aggregate_features] agg_mode=offï¼Œè·³è¿‡èšåˆ | ts_iso={ts_iso}")
            agg_result["aggregation_success"] = True
            agg_result["skipped"] = True
            agg_result["reason"] = "agg_mode=off"
            return agg_result

        # å¦‚æœä¸æ˜¯è¾¹ç•Œæ—¶é—´ç‚¹ï¼Œä¸æ‰§è¡Œèšåˆ
        if not is_boundary:
            logger.info(f"[psta_aggregate_features] éè¾¹ç•Œæ—¶é—´ç‚¹ï¼Œè·³è¿‡èšåˆ | ts_iso={ts_iso}")
            agg_result["aggregation_success"] = True
            agg_result["skipped"] = True
            agg_result["reason"] = "not_boundary"
            return agg_result

        logger.info(
            f"[psta_aggregate_features] å¼€å§‹èšåˆ | "
            f"ts_iso={ts_iso} | agg_minutes={agg_minutes} | "
            f"bucket_start={agg_start_iso}"
        )

        # æ‰§è¡Œèšåˆ
        ts_dt = _to_aware(ts_iso)
        _run_aggregation(
            ts_iso=ts_iso,
            ts_dt=ts_dt,
            rows=[],  # v3: èšåˆä»æ•°æ®åº“è¯»å–ï¼Œä¸å†ä¾èµ– rows å‚æ•°
            agg_start_iso=agg_start_iso,
            agg_minutes=agg_minutes,
        )

        agg_result["aggregation_success"] = True
        logger.info(f"[psta_aggregate_features] èšåˆå®Œæˆ | ts_iso={ts_iso}")

        # å¹¿æ’­èšåˆå®Œæˆé€šçŸ¥
        try:
            notify_progress_all(data={
                "type": "aggregation_complete",
                "ts_iso": ts_iso,
                "job_id": job_id,
                "agg_ctx": agg_ctx,
            })
        except Exception:
            pass

    except Exception as e:
        error_msg = f"èšåˆå¤±è´¥: {repr(e)}"
        logger.error(f"[psta_aggregate_features] {error_msg} | ts_iso={ts_iso}", exc_info=True)
        agg_result["error"] = error_msg

        # å¹¿æ’­èšåˆå¤±è´¥é€šçŸ¥
        try:
            notify_progress_all(data={
                "type": "aggregation_failed",
                "ts_iso": ts_iso,
                "job_id": job_id,
                "error": error_msg,
            })
        except Exception:
            pass

    return agg_result


# -----------------------------------------------
# å›è°ƒï¼šèšåˆæ‰€æœ‰åˆ†é’Ÿæ¡¶ï¼Œå¹¿æ’­æœ€ç»ˆ"done + å›¾è¡¨å¢é‡"ï¼ˆv3: è§¦å‘ç‹¬ç«‹èšåˆä»»åŠ¡ï¼‰
# -----------------------------------------------


@shared_task(
    name="AppleStockChecker.tasks.psta_finalize_buckets",
    queue="psta_finalize",
    soft_time_limit=240,
    time_limit=360,
)
def psta_finalize_buckets(
        results: List[Dict[str, Any]],
        job_id: str,
        ts_iso: str,
        agg_ctx: Optional[dict] = None,
        task_ver: Optional[int] = None,
        **_compat
) -> Dict[str, Any]:
    """
    å›è°ƒä»»åŠ¡ï¼šæ±‡æ€»æ‰€æœ‰åˆ†é’Ÿæ¡¶ç»“æœï¼Œå¹¿æ’­é€šçŸ¥ï¼Œå¹¶è§¦å‘èšåˆä»»åŠ¡ã€‚

    v3 å˜æ›´ï¼š
    - æ·»åŠ ç‹¬ç«‹é˜Ÿåˆ— psta_finalize
    - åœ¨è¾¹ç•Œæ—¶é—´ç‚¹åŒæ­¥è°ƒç”¨ psta_aggregate_features æ‰§è¡Œèšåˆ
    - èšåˆå¤±è´¥ä¸å½±å“æ•°æ®å†™å…¥ç»“æœçš„è¿”å›
    """
    from collections import defaultdict, Counter
    incoming = dict(results=results, job_id=job_id, ts_iso=ts_iso,
                    agg_ctx=agg_ctx, task_ver=task_ver, **_compat)
    normalized, meta = guard_params(
        "psta_finalize_buckets",
        incoming,
        required={"results": list, "job_id": str, "ts_iso": str},
        optional={"agg_ctx": (dict, type(None)), "task_ver": (int, str, type(None))},
        defaults={"agg_ctx": None},
        task_ver_field="task_ver",
        expected_ver=TASK_VER_PSTA,
        notify=notify_progress_all,
    )
    results = normalized["results"]
    job_id = normalized["job_id"]
    ts_iso = normalized["ts_iso"]
    agg_ctx = normalized.get("agg_ctx")

    # --- æ ‡å‡†åŒ– resultsï¼ˆæœ‰æ—¶ä¸æ˜¯ listï¼‰ ---
    if isinstance(results, dict):
        results = [results]
    elif results is None:
        results = []
    # === æ±‡æ€»è®¡æ•° ===
    total_buckets = len(results or [])
    total_ok = sum(int(r.get("ok", 0)) for r in results or [])
    total_failed = sum(int(r.get("failed", 0)) for r in results or [])

    # === é”™è¯¯ç›´æ–¹å›¾ ===
    agg_err = Counter()
    for r in results or []:
        for k, v in (r.get("error_hist") or {}).items():
            agg_err[k] += v

    # === èšåˆçœŸå®ç‚¹ ===
    # key: (iphone_id, shop_id) -> List[point]
    series_map = defaultdict(list)
    total_points = 0
    for r in results or []:
        for p in (r.get("chart_points") or []):
            key = (p.get("iphone_id"), p.get("shop_id"))
            series_map[key].append({
                "id": p.get("id"),
                "t": p.get("t"),
                "price": p.get("price"),
                "recorded_at": p.get("recorded_at"),
            })
            total_points += 1

    # === è®¡ç®—æ¯ä¸ªåºåˆ—åœ¨ ts_iso ä¹‹å‰ï¼ˆå«ï¼‰çš„æœ€åä¸€ä¸ªçœŸå®ç‚¹ï¼ˆlast-knownï¼‰ï¼Œä»¥åŠæ˜¯å¦åœ¨ ts_iso æœ‰çœŸå®ç‚¹ ===
    # ä¸ºé¿å…æ­§ä¹‰ï¼Œè¿™é‡Œä»¥ ISO å­—ç¬¦ä¸²çš„æ—¶é—´æ¯”è¾ƒä¸ºå‡†ï¼ˆä½ ä»¬ä¸Šä¸‹æ–‡é‡Œ t å’Œ ts_iso çš„æ ¼å¼ä¸€è‡´ï¼‰ã€‚
    # è‹¥æ‹…å¿ƒè·¨æ—¶åŒº ISO æ–‡æœ¬æ¯”è¾ƒçš„ç¨³å®šæ€§ï¼Œå¯æ”¹ä¸º _to_aware åš datetime æ¯”è¾ƒã€‚
    last_known = {}  # key -> dict(point)
    has_real_at_ts = {}  # key -> bool
    for key, pts in series_map.items():
        # æ‰¾ <= ts_iso çš„æœ€å¤§ t
        latest = None
        latest_t = None
        at_ts = False
        for item in pts:
            t_iso = item["t"]
            if t_iso == ts_iso:
                at_ts = True
            # é€‰æ‹© <= ts_iso ä¸­æœ€å¤§çš„ t
            if t_iso <= ts_iso and (latest_t is None or t_iso > latest_t):
                latest = item
                latest_t = t_iso
        if latest:
            last_known[key] = latest
        has_real_at_ts[key] = at_ts

    # === å…¨å±€æˆªæ–­ï¼ˆä»…å¯¹çœŸå®ç‚¹ç”Ÿæ•ˆï¼›å½±å­ç‚¹ä¸å— MAX_PUSH_POINTS é™åˆ¶ï¼‰ ===
    clipped = False
    if total_points > MAX_PUSH_POINTS:
        clipped = True
        flat = []
        for (iphone_id, shop_id), pts in series_map.items():
            for item in pts:
                flat.append((item["t"], iphone_id, shop_id, item))
        flat.sort(key=lambda x: x[0])  # å‡åº
        flat = flat[-MAX_PUSH_POINTS:]  # ä¿ç•™æœ€è¿‘ N æ¡

        series_map = defaultdict(list)
        for _, iphone_id, shop_id, item in flat:
            series_map[(iphone_id, shop_id)].append(item)

    # === ç”Ÿæˆæœ€ç»ˆå¢é‡ï¼šçœŸå®ç‚¹ +ï¼ˆå¿…è¦æ—¶ï¼‰å½±å­ç‚¹ ===
    series_delta = []
    shadow_points_added = 0
    # æ³¨æ„ï¼šç”¨æ‰€æœ‰å‡ºç°è¿‡çš„ keyï¼ˆåŒ…æ‹¬è¢«æˆªæ–­åçš„ç©ºç³»åˆ—ï¼Œä¿è¯å½±å­ç‚¹ä¹Ÿèƒ½å‡ºç°ï¼‰
    all_keys = set(last_known.keys()) | set(series_map.keys())

    for (iphone_id, shop_id) in all_keys:
        pts = series_map.get((iphone_id, shop_id), [])
        # ä¿è¯æ—¶é—´æœ‰åº
        pts.sort(key=lambda x: x["t"])

        # è‹¥è¯¥åºåˆ—åœ¨ ts_iso æ²¡æœ‰çœŸå®ç‚¹ï¼Œä½†æœ‰ last-knownï¼Œåˆ™è¡¥å½±å­ç‚¹
        if not has_real_at_ts.get((iphone_id, shop_id), False) and (iphone_id, shop_id) in last_known:
            src = last_known[(iphone_id, shop_id)]
            # é¿å…ä¸çœŸå®ç‚¹é‡å¤ï¼ˆç†è®ºä¸Š has_real_at_ts å·²æ’é™¤ï¼‰
            if not any(p["t"] == ts_iso for p in pts):
                shadow_points_added += 1
                pts.append({
                    "id": None,  # å½±å­ç‚¹ä¸è½åº“ï¼Œæ—  id
                    "t": ts_iso,  # å½±å­ç‚¹æ”¾åœ¨æ ‡çš„æ—¶é—´æˆ³
                    "price": src["price"],  # ä»¥æœ€è¿‘çœŸå®ç‚¹çš„ä»·æ ¼å¡«å……
                    "recorded_at": src.get("recorded_at"),
                    "shadow": True,  # âœ… æ ‡è¯†å½±å­ç‚¹
                    "src_t": src["t"],  # å½±å­æ¥æºæ—¶é—´ï¼ˆä¾¿äºå‰ç«¯ tooltip/æ ·å¼ï¼‰
                })

        series_delta.append({
            "iphone_id": iphone_id,
            "shop_id": shop_id,
            "points": pts,  # [{id,t,price,recorded_at,shadow?,src_t?}, ...]
        })

    # === æ„å»ºæ±‡æ€»ä¸å¹¿æ’­ payload ===
    summary = {
        "timestamp": ts_iso,
        "job_id": job_id,
        "total_buckets": total_buckets,
        "ok": total_ok,
        "failed": total_failed,
        "error_hist": dict(agg_err),
        "by_bucket": [
            {k: r.get(k) for k in ("ts_iso", "ok", "failed", "total", "error_hist")}
            for r in (results or [])
        ][:100],
    }

    payload = {
        "status": "done",
        "step": "finalize",
        "progress": 100,
        "summary": summary,
        "chart_delta": {
            "job_id": job_id,
            "timestamp": ts_iso,
            "series_delta": series_delta,
            "meta": {
                "total_points": min(total_points, MAX_PUSH_POINTS),  # ä»…çœŸå®ç‚¹è®¡æ•°
                "shadow_points": shadow_points_added,  # æœ¬æ¬¡è¡¥çš„å½±å­ç‚¹æ•°
                "clipped": clipped,
            }
        }
    }

    try:
        notify_progress_all(data=payload)
    except Exception:
        pass

    # ========== v3: è§¦å‘èšåˆä»»åŠ¡ ==========
    agg_result = None
    if agg_ctx:
        agg_mode = (agg_ctx.get("agg_mode") or "boundary").lower()
        is_boundary = agg_ctx.get("is_boundary", False)

        # ä»…åœ¨é off æ¨¡å¼ä¸”ä¸ºè¾¹ç•Œæ—¶é—´ç‚¹æ—¶è§¦å‘èšåˆ
        if agg_mode != "off" and is_boundary:
            logger.info(
                f"[psta_finalize_buckets] è§¦å‘èšåˆä»»åŠ¡ | "
                f"ts_iso={ts_iso} | is_boundary={is_boundary}"
            )
            try:
                # åŒæ­¥è°ƒç”¨èšåˆä»»åŠ¡å¹¶ç­‰å¾…ç»“æœ
                agg_result = psta_aggregate_features.apply(
                    kwargs={
                        "ts_iso": ts_iso,
                        "job_id": job_id,
                        "agg_ctx": agg_ctx,
                        "task_ver": TASK_VER_PSTA,
                    }
                ).get(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶

                if agg_result and not agg_result.get("aggregation_success"):
                    logger.error(
                        f"[psta_finalize_buckets] èšåˆä»»åŠ¡è¿”å›å¤±è´¥ | "
                        f"ts_iso={ts_iso} | error={agg_result.get('error')}"
                    )
            except Exception as e:
                logger.error(
                    f"[psta_finalize_buckets] èšåˆä»»åŠ¡æ‰§è¡Œå¼‚å¸¸ | "
                    f"ts_iso={ts_iso} | error={repr(e)}",
                    exc_info=True
                )
                agg_result = {
                    "aggregation_success": False,
                    "error": repr(e),
                }
        else:
            logger.info(
                f"[psta_finalize_buckets] è·³è¿‡èšåˆ | "
                f"agg_mode={agg_mode} | is_boundary={is_boundary}"
            )

    # å°†èšåˆç»“æœé™„åŠ åˆ° payload
    payload["aggregation"] = agg_result

    return payload


# -----------------------------------------------------
# --------------------------------------------------------
# -----------------------------------------------------------
# ---------------------------------------------------------------
# -----------------------------------------------------------
# --------------------------------------------------------
# -----------------------------------------------------
# -----------------------------------------------
# çˆ¶ä»»åŠ¡ï¼šchord å¹¶è¡Œ + å›è°ƒï¼ˆä¿æŒä½ å·²æœ‰å†™æ³•ï¼‰
# -----------------------------------------------


def _to_aware(s: str) -> timezone.datetime:
    from django.utils.dateparse import parse_datetime
    from django.utils.timezone import make_aware, is_naive
    dt = parse_datetime(s)
    if dt is None:
        raise ValueError(f"bad datetime iso: {s}")
    return make_aware(dt) if is_naive(dt) else dt


def _floor_to_step(dt: timezone.datetime, step_min: int) -> timezone.datetime:
    return dt - timezone.timedelta(minutes=dt.minute % step_min, seconds=dt.second, microseconds=dt.microsecond)


def _rolling_start(dt: timezone.datetime, step_min: int) -> timezone.datetime:
    return dt.replace(second=0, microsecond=0) - timezone.timedelta(minutes=max(step_min - 1, 0))


@shared_task(bind=True, name="AppleStockChecker.tasks.batch_generate_psta_same_ts")
def batch_generate_psta_same_ts(
        self,
        *,
        job_id: Optional[str] = None,
        items: Optional[List[Dict[str, Any]]] = None,
        timestamp_iso: Optional[str] = None,
        query_window_minutes: int = 15,
        shop_ids: Optional[List[int]] = None,
        iphone_ids: Optional[List[int]] = None,
        max_items: Optional[int] = None,
        # èšåˆæ§åˆ¶
        agg_minutes: int = 15,  # èšåˆæ­¥é•¿
        agg_mode: str = "boundary",  # 'off' ç¦ç”¨èšåˆï¼Œå…¶ä»–å€¼ç»Ÿä¸€ä¸ºè¾¹ç•Œæ¨¡å¼
        sequential: bool = False,  # æ˜¯å¦é¡ºåºæ‰§è¡Œå­ä»»åŠ¡
        **_compat,  # å‘åå…¼å®¹ï¼šæ¥å—å·²åºŸå¼ƒå‚æ•° chunk_size, force_agg
) -> Dict[str, Any]:
    """
    çˆ¶ä»»åŠ¡ï¼šåˆ†å‘åˆ†é’Ÿæ¡¶å­ä»»åŠ¡ï¼Œæ±‡æ€»ç»“æœã€‚

    v3 å˜æ›´ï¼š
    - ç§»é™¤ force_agg å‚æ•°ï¼ˆå·²åºŸå¼ƒï¼‰
    - agg_mode ç®€åŒ–ï¼šoff ç¦ç”¨èšåˆï¼Œå…¶ä»–å€¼ç»Ÿä¸€ä¸ºè¾¹ç•Œæ¨¡å¼
    - èšåˆé€»è¾‘ç§»è‡³ psta_finalize_buckets
    - å­ä»»åŠ¡ä¸å†ä¼ é€’ do_agg, agg_start_iso å‚æ•°
    """
    # æ£€æŸ¥åºŸå¼ƒå‚æ•°
    if "force_agg" in _compat:
        logger.warning("[batch_generate_psta_same_ts] force_agg å‚æ•°å·²åºŸå¼ƒ(v3)ï¼Œå°†è¢«å¿½ç•¥")
    if "chunk_size" in _compat:
        logger.warning("[batch_generate_psta_same_ts] chunk_size å‚æ•°å·²åºŸå¼ƒï¼Œå°†è¢«å¿½ç•¥")

    task_job_id = job_id or self.request.id
    ts_iso = timestamp_iso or nearest_past_minute_iso()

    # v3: agg_mode ç®€åŒ– - off ç¦ç”¨ï¼Œå…¶ä»–ç»Ÿä¸€ä¸ºè¾¹ç•Œæ¨¡å¼
    MODE = (agg_mode or "boundary").lower()
    if MODE not in ("off", "boundary"):
        logger.info(f"[batch_generate_psta_same_ts] agg_mode='{agg_mode}' å°†ä½œä¸ºè¾¹ç•Œæ¨¡å¼å¤„ç†")
        MODE = "boundary"

    pack = (collect_items_for_psta(
        window_minutes=query_window_minutes,
        timestamp_iso=ts_iso,
        shop_ids=shop_ids,
        iphone_ids=iphone_ids,
        max_items=max_items,
    ) or [{}])[0]

    rows = pack.get("rows") or []
    bucket_minute_key: Dict[str, Dict[str, List[int]]] = pack.get("bucket_minute_key") or {}

    # è®¡ç®—è¾¹ç•Œæ—¶é—´å’Œ is_boundary æ ‡å¿—
    dt0 = _to_aware(ts_iso)
    step0 = _floor_to_step(dt0, int(agg_minutes))
    is_boundary = (dt0 == step0)

    # æ„å»ºèšåˆä¸Šä¸‹æ–‡ï¼ˆä¼ é€’ç»™ finalizeï¼‰
    ctx = {
        "agg_minutes": int(agg_minutes),
        "agg_mode": MODE,
        "is_boundary": is_boundary,  # v3: æ–°å¢è¾¹ç•Œæ ‡å¿—
        "bucket_start": step0.isoformat(),
        "bucket_end": (step0 + timezone.timedelta(minutes=int(agg_minutes))).isoformat(),
    }

    try:
        notify_progress_all(data={"type": "agg_ctx", "timestamp": ts_iso, "job_id": task_job_id, "ctx": ctx})
    except Exception:
        pass

    # æ„å»ºå­ä»»åŠ¡ï¼ˆv3: å­ä»»åŠ¡ä»…è´Ÿè´£å†™å…¥æ•°æ®ï¼Œä¸åšèšåˆï¼‰
    subtasks: List = []
    for minute_iso, key_map in bucket_minute_key.items():
        minute_rows: List[Dict[str, Any]] = []
        for _, idx_list in (key_map or {}).items():
            for i in idx_list:
                if 0 <= i < len(rows):
                    r = rows[i]
                    minute_rows.append({
                        "shop_id": r.get("shop_id"),
                        "iphone_id": r.get("iphone_id"),
                        "recorded_at": r.get("recorded_at"),
                        "price_new": r.get("price_new", r.get("New_Product_Price")),
                    })

        # v3: åªè¦æœ‰æ•°æ®å°±ä¸‹å‘å­ä»»åŠ¡ï¼ˆèšåˆç”± finalize ç»Ÿä¸€å¤„ç†ï¼‰
        if minute_rows:
            subtasks.append(
                psta_process_minute_bucket.s(
                    ts_iso=minute_iso,
                    rows=minute_rows,
                    job_id=task_job_id,
                    agg_minutes=int(agg_minutes),  # ä¿ç•™ç”¨äºæ—¥å¿—
                    task_ver=TASK_VER_PSTA,
                )
            )

    try:
        notify_progress_all(
            data={"status": "running", "step": "dispatch_buckets", "buckets": len(subtasks), "timestamp": ts_iso,
                  "agg": ctx})
    except Exception:
        pass

    if not subtasks:
        empty = {"timestamp": ts_iso, "total_buckets": 0, "ok": 0, "failed": 0, "by_bucket": []}
        try:
            notify_progress_all(data={
                "status": "done", "progress": 100, "summary": empty,
                "chart_delta": {"job_id": task_job_id, "timestamp": ts_iso, "series_delta": [],
                                "meta": {"total_points": 0, "shadow_points": 0, "clipped": False}}
            })
        except Exception:
            pass
        return empty

    # æ ¹æ® sequential å‚æ•°é€‰æ‹©æ‰§è¡Œæ–¹å¼
    if sequential:
        # é¡ºåºæ‰§è¡Œæ¨¡å¼ï¼šé€ä¸ªæ‰§è¡Œå­ä»»åŠ¡å¹¶æ”¶é›†ç»“æœ
        results = []
        for i, subtask in enumerate(subtasks):
            try:
                # åŒæ­¥è°ƒç”¨å­ä»»åŠ¡ï¼ˆé˜»å¡ç­‰å¾…å®Œæˆï¼‰
                result = subtask.apply().get()
                results.append(result)

                # å¯é€‰ï¼šæŠ¥å‘Šè¿›åº¦
                try:
                    notify_progress_all(
                        data={
                            "status": "running",
                            "step": f"processing_bucket_{i+1}",
                            "progress": int((i + 1) * 100 / len(subtasks)),
                            "current": i + 1,
                            "total": len(subtasks),
                            "timestamp": ts_iso,
                        }
                    )
                except Exception:
                    pass
            except Exception as e:
                # è®°å½•é”™è¯¯ä½†ç»§ç»­æ‰§è¡Œ
                results.append({
                    "ok": 0,
                    "failed": 1,
                    "error": str(e),
                    "error_hist": {"sequential_execution_error": 1},
                })

        # ç›´æ¥è°ƒç”¨ finalize å¤„ç†ç»“æœ
        final_result = psta_finalize_buckets(
            results=results,
            job_id=task_job_id,
            ts_iso=ts_iso,
            agg_ctx=ctx,
            task_ver=TASK_VER_PSTA,
        )
        return {
            "timestamp": ts_iso,
            "total_buckets": len(subtasks),
            "job_id": task_job_id,
            "sequential": True,
            "result": final_result,
        }
    else:
        # å¹¶å‘æ‰§è¡Œæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰ï¼šä½¿ç”¨ chord
        callback = psta_finalize_buckets.s(job_id=task_job_id, ts_iso=ts_iso, agg_ctx=ctx,
                                           task_ver=TASK_VER_PSTA)  # å¯æŠŠ ctx ä¼ ç»™å›è°ƒï¼ˆå¯é€‰ï¼‰
        chord_result = chord(subtasks)(callback)
        return {"timestamp": ts_iso, "total_buckets": len(subtasks), "job_id": task_job_id, "chord_id": chord_result.id}

# -----------------------------------------------------
# --------------------------------------------------------
# -----------------------------------------------------------
# ---------------------------------------------------------------
# -----------------------------------------------------------
# --------------------------------------------------------
# -----------------------------------------------------
