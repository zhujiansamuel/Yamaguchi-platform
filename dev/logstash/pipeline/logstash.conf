input {
  beats {
    port => 5044
  }
}

filter {
  # Extract container ID from log file path
  if [log][file][path] {
    grok {
      match => { "[log][file][path]" => "/var/lib/docker/containers/%{DATA:container_id}/%{GREEDYDATA}" }
    }

    # Get short container ID (first 12 chars)
    if [container_id] {
      mutate {
        gsub => [ "container_id", "^(.{12}).*", "\1" ]
      }

      # Translate container ID to container name
      translate {
        source => "container_id"
        target => "container_name"
        dictionary_path => "/usr/share/logstash/mappings/container-mapping.yml"
        refresh_interval => 60
        fallback => "unknown"
      }
    }
  }

  # Parse JSON logs if applicable
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
      target => "parsed_json"
      skip_on_invalid_json => true
    }
  }

  # Parse Django/Daphne logs
  if [message] =~ /^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}/ {
    grok {
      match => { "message" => "%{IP:client_ip}:%{INT:client_port} - - \[%{DATA:timestamp}\] \"%{WORD:http_method} %{URIPATHPARAM:request_path}\" %{INT:http_status:int} %{INT:response_size:int}" }
      tag_on_failure => ["_grokparsefailure_django"]
    }
  }

  # Parse Celery worker logs
  if [message] =~ /\[.*\/(INFO|WARNING|ERROR|DEBUG|CRITICAL)\]/ {
    grok {
      match => { "message" => "\[%{TIMESTAMP_ISO8601:log_timestamp}\] \[%{DATA:task_id}\/%{LOGLEVEL:log_level}\] %{GREEDYDATA:log_message}" }
      tag_on_failure => ["_grokparsefailure_celery"]
    }
  }

  # Add timestamp
  if [timestamp] {
    date {
      match => [ "timestamp", "ISO8601", "dd/MMM/YYYY:HH:mm:ss" ]
      target => "@timestamp"
    }
  }

  if [log_timestamp] {
    date {
      match => [ "log_timestamp", "ISO8601" ]
      target => "@timestamp"
    }
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "docker-logs-%{+YYYY.MM.dd}"
  }

  # Uncomment for debugging
  # stdout { codec => rubydebug }
}
